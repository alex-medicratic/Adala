{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart","text":"<p>Adala is an Autonomous DAta (Labeling) Agent framework.</p> <p>Adala offers a robust framework for implementing agents specialized in data processing, with an emphasis on diverse data labeling tasks. These agents are autonomous, meaning they can independently acquire one or more skills through iterative learning. This learning process is influenced by their operating environment, observations, and reflections. Users define the environment by providing a ground truth dataset. Every agent learns and applies its skills in what we refer to as a \"runtime\", synonymous with LLM.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Install Adala:</p> <pre><code>pip install adala\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Set OPENAI_API_KEY (see instructions here)</p>"},{"location":"#quickstart_1","title":"\ud83c\udfac Quickstart","text":"<p>In this example we will use Adala as a standalone library directly inside Python notebook.</p> <p>Click here to see an extended quickstart example. </p> <pre><code>import pandas as pd\n\nfrom adala.agents import Agent\nfrom adala.datasets import DataFrameDataset\nfrom adala.environments import BasicEnvironment\nfrom adala.skills import ClassificationSkill\nfrom adala.runtimes import OpenAIRuntime\nfrom rich import print\n\n# Train dataset\nground_truth_df = pd.DataFrame([\n    [\"It was the negative first impressions, and then it started working.\", \"Positive\"],\n    [\"Not loud enough and doesn't turn on like it should.\", \"Negative\"],\n    [\"I don't know what to say.\", \"Neutral\"],\n    [\"Manager was rude, but the most important that mic shows very flat frequency response.\", \"Positive\"],\n    [\"The phone doesn't seem to accept anything except CBR mp3s.\", \"Negative\"],\n    [\"I tried it before, I bought this device for my son.\", \"Neutral\"],\n], columns=[\"text\", \"ground_truth\"])\n\n# Test dataset\npredict_df = pd.DataFrame([\n    \"All three broke within two months of use.\",\n    \"The device worked for a long time, can't say anything bad.\",\n    \"Just a random line of text.\"\n], columns=[\"text\"])\n\nground_truth_dataset = DataFrameDataset(df=ground_truth_df)\npredict_dataset = DataFrameDataset(df=predict_df)\n\nagent = Agent(\n    # connect to a dataset\n    environment=BasicEnvironment(\n        ground_truth_dataset=ground_truth_dataset,\n        ground_truth_columns={\"sentiment_classification\": \"ground_truth\"}\n    ),\n\n    # define a skill\n    skills=ClassificationSkill(\n        name='sentiment_classification',\n        instructions=\"Label text as positive, negative or neutral.\",\n        labels=[\"Positive\", \"Negative\", \"Neutral\"],\n        input_data_field='text'\n    ),\n\n    # define all the different runtimes your skills may use\n    runtimes = {\n        # You can specify your OPENAI API KEY here via `OpenAIRuntime(..., api_key='your-api-key')`\n        'openai': OpenAIRuntime(model='gpt-3.5-turbo-instruct'),\n        'openai-gpt3': OpenAIRuntime(model='gpt-3.5-turbo')\n    },\n    default_runtime='openai',\n\n    # NOTE! If you have access to GPT-4, you can uncomment the lines bellow for better results\n#     default_teacher_runtime='openai-gpt4',\n#     teacher_runtimes = {\n#       'openai-gpt4': OpenAIRuntime(model='gpt-4')\n#     }\n)\n\nprint(agent)\nprint(agent.skills)\n\nagent.learn(learning_iterations=3, accuracy_threshold=0.95)\n\nprint('\\n=&gt; Run tests ...')\npredictions = agent.run(predict_dataset)\nprint('\\n =&gt; Test results:')\nprint(predictions)\n</code></pre>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Agents - main interface for  interacting with environment</li> <li>Datasets - data inputs for agents</li> <li>Environments - environments for agents, where it collects ground truth signal</li> <li>Memories - agent's memory for storing and retrieving data</li> <li>Runtimes - agent's execution runtime (e.g. LLMs providers)</li> <li>Skills - agent skills for data labeling</li> </ul>"},{"location":"agents/","title":"Agents","text":""},{"location":"agents/#adala.agents.base.Agent","title":"<code>Agent</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a customizable agent that can interact with environments,  employ skills, and leverage memory and runtimes.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Union[Dataset, Environment]</code> <p>The environment with which the agent interacts.</p> <code>skills</code> <code>Union[SkillSet, BaseSkill, List[BaseSkill], Dict[str, BaseSkill]]</code> <p>The skills possessed by the agent.</p> <code>memory</code> <code>LongTermMemory</code> <p>The agent's long-term memory. Defaults to None.</p> <code>runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent. Defaults to predefined runtimes.</p> <code>default_runtime</code> <code>str</code> <p>The default runtime used by the agent. Defaults to 'openai'.</p> <code>teacher_runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent's teacher. Defaults to predefined runtimes.</p> <code>default_teacher_runtime</code> <code>str</code> <p>The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.</p> Source code in <code>adala/agents/base.py</code> <pre><code>class Agent(BaseModel, ABC):\n    \"\"\"\n    Represents a customizable agent that can interact with environments, \n    employ skills, and leverage memory and runtimes.\n\n    Attributes:\n        environment (Union[Dataset, Environment]): The environment with which the agent interacts.\n        skills (Union[SkillSet, BaseSkill, List[BaseSkill], Dict[str, BaseSkill]]): The skills possessed by the agent.\n        memory (LongTermMemory, optional): The agent's long-term memory. Defaults to None.\n        runtimes (Dict[str, Runtime], optional): The runtimes available to the agent. Defaults to predefined runtimes.\n        default_runtime (str): The default runtime used by the agent. Defaults to 'openai'.\n        teacher_runtimes (Dict[str, Runtime], optional): The runtimes available to the agent's teacher. Defaults to predefined runtimes.\n        default_teacher_runtime (str): The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.\n    \"\"\"\n\n    environment: Union[InternalDataFrame, Dataset, Environment] = Field(default_factory=DataFrameDataset)\n    skills: SkillSet\n\n    memory: Memory = Field(default=None)\n    runtimes: Optional[Dict[str, Runtime]] = Field(\n        default_factory=lambda: {\n            'openai': OpenAIRuntime(model='gpt-3.5-turbo-instruct'),\n            # 'llama2': LLMRuntime(\n            #     llm_runtime_type=LLMRuntimeModelType.Transformers,\n            #     llm_params={\n            #         'model': 'meta-llama/Llama-2-7b',\n            #         'device': 'cuda:0',\n            #     }\n            # )\n        }\n    )\n    teacher_runtimes: Optional[Dict[str, Runtime]] = Field(\n        default_factory=lambda: {\n            'openai-gpt3': OpenAIRuntime(model='gpt-3.5-turbo'),\n            # 'openai-gpt4': OpenAIRuntime(model='gpt-4')\n        }\n    )\n    default_runtime: str = 'openai'\n    default_teacher_runtime: str = 'openai-gpt3'\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __rich__(self) -&gt; str:\n        \"\"\"\n        Returns a colorized and formatted representation of the Agent instance.\n\n        Returns:\n            str: A rich-formatted representation of the agent.\n        \"\"\"\n\n        skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n        runtime_names = \", \".join(self.runtimes.keys())\n\n        return (\n            f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n            f\"Environment: {self.environment.__class__.__name__}\\n\"\n            f\"Skills: {skill_names}\\n\"\n            f\"Runtimes: {runtime_names}\\n\"\n            f\"Default Runtime: {self.default_runtime}\\n\"\n            f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n        )\n\n    @field_validator('environment')\n    def environment_validator(cls, v) -&gt; Environment:\n        \"\"\"\n        Validates and possibly transforms the environment attribute.\n\n        Args:\n            v (Union[Dataset, Environment]): The environment value to validate.\n\n        Returns:\n            Environment: The validated environment.\n        \"\"\"\n        if isinstance(v, InternalDataFrame):\n            v = DataFrameDataset(df=v)\n        if isinstance(v, Dataset):\n            v = BasicEnvironment(dataset=v)\n        return v\n\n    @field_validator('skills', mode='before')\n    def skills_validator(cls, v) -&gt; SkillSet:\n        \"\"\"\n        Validates and possibly transforms the skills attribute.\n\n        Args:\n            v (Union[SkillSet, BaseSkill, List[BaseSkill], Dict[str, BaseSkill]]): The skills value to validate.\n\n        Returns:\n            SkillSet: The validated set of skills.\n        \"\"\"\n\n        if isinstance(v, SkillSet):\n            return v\n        elif isinstance(v, BaseSkill):\n            return LinearSkillSet(skills={v.name: v})\n        else:\n            return LinearSkillSet(skills=v)\n\n    @model_validator(mode='after')\n    def verify_input_parameters(self):\n        def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n            print_error(f\"The Agent.{runtime} is set to {val}, \"\n                        f\"but this runtime is not available in the list: {list(runtimes)}. \"\n                        f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n                        f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n                        f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n                        f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\")\n            raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n        if self.default_runtime not in self.runtimes:\n            _raise_default_runtime_error(self.default_runtime, 'default_runtime', self.runtimes, 'openai')\n        if self.default_teacher_runtime not in self.teacher_runtimes:\n            _raise_default_runtime_error(self.default_teacher_runtime, 'default_teacher_runtime', self.teacher_runtimes, 'openai-gpt4')\n        return self\n\n    def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_runtime\n        if runtime not in self.runtimes:\n            raise ValueError(f'Runtime \"{runtime}\" not found.')\n        return self.runtimes[runtime]\n\n    def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_teacher_runtime\n        if runtime not in self.teacher_runtimes:\n            raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n        return self.teacher_runtimes[runtime]\n\n    def run(\n        self,\n        dataset: Optional[Union[Dataset, InternalDataFrame]] = None,\n        runtime: Optional[str] = None\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Runs the agent on the specified dataset.\n\n        Args:\n            dataset (Union[Dataset, InternalDataFrame]): The dataset to run the agent on.\n            runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n        Returns:\n            InternalDataFrame: The dataset with the agent's predictions.\n        \"\"\"\n        if dataset is None:\n            dataset = self.environment.as_dataset()\n        runtime = self.get_runtime(runtime=runtime)\n        predictions = self.skills.apply(dataset, runtime=runtime)\n        return predictions\n\n    def learn(\n        self,\n        learning_iterations: int = 3,\n        accuracy_threshold: float = 0.9,\n        update_memory: bool = True,\n        request_environment_feedback: bool = True,\n        wait_for_environment_feedback: Optional[float] = None,\n        num_predictions_feedback: Optional[int] = None,\n        runtime: Optional[str] = None,\n        teacher_runtime: Optional[str] = None,\n    ) -&gt; GroundTruthSignal:\n        \"\"\"\n        Enables the agent to learn and improve its skills based on interactions with its environment.\n\n        Args:\n            learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n            accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n            update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n            request_environment_feedback (bool, optional): Flag to determine if feedback should be requested from the environment. Defaults to True.\n            wait_for_environment_feedback (float, optional): The timeout in seconds to wait for environment feedback. Defaults to None.\n            num_predictions_feedback (int, optional): The number of predictions to request feedback for. Defaults to None.\n            runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n            teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n        Returns:\n            GroundTruthSignal: The ground truth signal.\n        \"\"\"\n\n        runtime = self.get_runtime(runtime=runtime)\n        teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n        dataset = self.environment.as_dataset()\n\n        # Apply agent skills to dataset and get experience with predictions\n        predictions = self.skills.apply(dataset, runtime=runtime)\n\n        ground_truth_signal = None\n\n        for iteration in range(learning_iterations):\n            print_text(f'\\n\\n=&gt; Iteration #{iteration}: Comparing to ground truth, analyzing and improving ...')\n\n            # Request feedback from environment is necessary\n            if request_environment_feedback:\n                if num_predictions_feedback is not None:\n                    # predictions_for_feedback = predictions.sample(num_predictions_feedback)\n                    predictions_for_feedback = predictions.head(num_predictions_feedback)\n                else:\n                    predictions_for_feedback = predictions\n                self.environment.request_feedback(self.skills, predictions_for_feedback)\n\n            # Compare predictions to ground truth -&gt; get ground truth signal\n            ground_truth_signal = self.environment.compare_to_ground_truth(\n                self.skills,\n                predictions,\n                wait=wait_for_environment_feedback\n            )\n\n            print_text(f'Comparing predictions to ground truth data ...')\n            print_dataframe(InternalDataFrameConcat([predictions, ground_truth_signal.match], axis=1))\n\n            # Use ground truth signal to find the skill to improve\n            accuracy = ground_truth_signal.get_accuracy()\n            train_skill = self.skills.select_skill_to_improve(accuracy, accuracy_threshold)\n            if not train_skill:\n                print_text(f'No skill to improve found. Stopping learning process.')\n                break\n            # select the worst performing skill\n            print_text(f'Accuracy = {accuracy[train_skill.name] * 100:0.2f}%', style='bold red')\n\n            skill_errors = ground_truth_signal.get_errors(train_skill.name)\n\n            # 2. ANALYSIS PHASE: Analyze evaluation experience, optionally use long term memory\n            print_text(f'Analyze evaluation experience ...')\n            error_analysis = train_skill.analyze(\n                predictions=predictions,\n                errors=skill_errors,\n                student_runtime=runtime,\n                teacher_runtime=teacher_runtime,\n                memory=self.memory\n            )\n            print_text(f'Error analysis for skill \"{train_skill.name}\":\\n')\n            print_text(error_analysis, style='green')\n            if self.memory and update_memory:\n                self.memory.remember(error_analysis, self.skills)\n\n            # 3. IMPROVEMENT PHASE: Improve skills based on analysis\n            print_text(f\"Improve \\\"{train_skill.name}\\\" skill based on analysis ...\")\n            train_skill.improve(\n                error_analysis=error_analysis,\n                runtime=teacher_runtime,\n            )\n            print_text(f'Updated instructions for skill \"{train_skill.name}\":\\n')\n            print_text(train_skill.instructions, style='bold green')\n\n            # 4. RE-APPLY PHASE: Re-apply skills to dataset\n            print_text(f\"Re-apply {train_skill.name} skill to dataset ...\")\n            self.skills[train_skill.name] = train_skill\n            predictions = self.skills.apply(predictions, runtime=runtime, improved_skill=train_skill.name)\n\n        print_text('Train is done!')\n        return ground_truth_signal\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a colorized and formatted representation of the Agent instance.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A rich-formatted representation of the agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def __rich__(self) -&gt; str:\n    \"\"\"\n    Returns a colorized and formatted representation of the Agent instance.\n\n    Returns:\n        str: A rich-formatted representation of the agent.\n    \"\"\"\n\n    skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n    runtime_names = \", \".join(self.runtimes.keys())\n\n    return (\n        f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n        f\"Environment: {self.environment.__class__.__name__}\\n\"\n        f\"Skills: {skill_names}\\n\"\n        f\"Runtimes: {runtime_names}\\n\"\n        f\"Default Runtime: {self.default_runtime}\\n\"\n        f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n    )\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.environment_validator","title":"<code>environment_validator(v)</code>","text":"<p>Validates and possibly transforms the environment attribute.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[Dataset, Environment]</code> <p>The environment value to validate.</p> required <p>Returns:</p> Name Type Description <code>Environment</code> <code>Environment</code> <p>The validated environment.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator('environment')\ndef environment_validator(cls, v) -&gt; Environment:\n    \"\"\"\n    Validates and possibly transforms the environment attribute.\n\n    Args:\n        v (Union[Dataset, Environment]): The environment value to validate.\n\n    Returns:\n        Environment: The validated environment.\n    \"\"\"\n    if isinstance(v, InternalDataFrame):\n        v = DataFrameDataset(df=v)\n    if isinstance(v, Dataset):\n        v = BasicEnvironment(dataset=v)\n    return v\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_runtime","title":"<code>get_runtime(runtime=None)</code>","text":"<p>Retrieves the specified runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_runtime\n    if runtime not in self.runtimes:\n        raise ValueError(f'Runtime \"{runtime}\" not found.')\n    return self.runtimes[runtime]\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_teacher_runtime","title":"<code>get_teacher_runtime(runtime=None)</code>","text":"<p>Retrieves the specified teacher runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_teacher_runtime\n    if runtime not in self.teacher_runtimes:\n        raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n    return self.teacher_runtimes[runtime]\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.learn","title":"<code>learn(learning_iterations=3, accuracy_threshold=0.9, update_memory=True, request_environment_feedback=True, wait_for_environment_feedback=None, num_predictions_feedback=None, runtime=None, teacher_runtime=None)</code>","text":"<p>Enables the agent to learn and improve its skills based on interactions with its environment.</p> <p>Parameters:</p> Name Type Description Default <code>learning_iterations</code> <code>int</code> <p>The number of iterations for learning. Defaults to 3.</p> <code>3</code> <code>accuracy_threshold</code> <code>float</code> <p>The desired accuracy threshold to reach. Defaults to 0.9.</p> <code>0.9</code> <code>update_memory</code> <code>bool</code> <p>Flag to determine if memory should be updated after learning. Defaults to True.</p> <code>True</code> <code>request_environment_feedback</code> <code>bool</code> <p>Flag to determine if feedback should be requested from the environment. Defaults to True.</p> <code>True</code> <code>wait_for_environment_feedback</code> <code>float</code> <p>The timeout in seconds to wait for environment feedback. Defaults to None.</p> <code>None</code> <code>num_predictions_feedback</code> <code>int</code> <p>The number of predictions to request feedback for. Defaults to None.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The runtime to be used for the learning process. Defaults to None.</p> <code>None</code> <code>teacher_runtime</code> <code>str</code> <p>The teacher runtime to be used for the learning process. Defaults to None.</p> <code>None</code> <p>Returns:     GroundTruthSignal: The ground truth signal.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def learn(\n    self,\n    learning_iterations: int = 3,\n    accuracy_threshold: float = 0.9,\n    update_memory: bool = True,\n    request_environment_feedback: bool = True,\n    wait_for_environment_feedback: Optional[float] = None,\n    num_predictions_feedback: Optional[int] = None,\n    runtime: Optional[str] = None,\n    teacher_runtime: Optional[str] = None,\n) -&gt; GroundTruthSignal:\n    \"\"\"\n    Enables the agent to learn and improve its skills based on interactions with its environment.\n\n    Args:\n        learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n        accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n        update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n        request_environment_feedback (bool, optional): Flag to determine if feedback should be requested from the environment. Defaults to True.\n        wait_for_environment_feedback (float, optional): The timeout in seconds to wait for environment feedback. Defaults to None.\n        num_predictions_feedback (int, optional): The number of predictions to request feedback for. Defaults to None.\n        runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n        teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n    Returns:\n        GroundTruthSignal: The ground truth signal.\n    \"\"\"\n\n    runtime = self.get_runtime(runtime=runtime)\n    teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n    dataset = self.environment.as_dataset()\n\n    # Apply agent skills to dataset and get experience with predictions\n    predictions = self.skills.apply(dataset, runtime=runtime)\n\n    ground_truth_signal = None\n\n    for iteration in range(learning_iterations):\n        print_text(f'\\n\\n=&gt; Iteration #{iteration}: Comparing to ground truth, analyzing and improving ...')\n\n        # Request feedback from environment is necessary\n        if request_environment_feedback:\n            if num_predictions_feedback is not None:\n                # predictions_for_feedback = predictions.sample(num_predictions_feedback)\n                predictions_for_feedback = predictions.head(num_predictions_feedback)\n            else:\n                predictions_for_feedback = predictions\n            self.environment.request_feedback(self.skills, predictions_for_feedback)\n\n        # Compare predictions to ground truth -&gt; get ground truth signal\n        ground_truth_signal = self.environment.compare_to_ground_truth(\n            self.skills,\n            predictions,\n            wait=wait_for_environment_feedback\n        )\n\n        print_text(f'Comparing predictions to ground truth data ...')\n        print_dataframe(InternalDataFrameConcat([predictions, ground_truth_signal.match], axis=1))\n\n        # Use ground truth signal to find the skill to improve\n        accuracy = ground_truth_signal.get_accuracy()\n        train_skill = self.skills.select_skill_to_improve(accuracy, accuracy_threshold)\n        if not train_skill:\n            print_text(f'No skill to improve found. Stopping learning process.')\n            break\n        # select the worst performing skill\n        print_text(f'Accuracy = {accuracy[train_skill.name] * 100:0.2f}%', style='bold red')\n\n        skill_errors = ground_truth_signal.get_errors(train_skill.name)\n\n        # 2. ANALYSIS PHASE: Analyze evaluation experience, optionally use long term memory\n        print_text(f'Analyze evaluation experience ...')\n        error_analysis = train_skill.analyze(\n            predictions=predictions,\n            errors=skill_errors,\n            student_runtime=runtime,\n            teacher_runtime=teacher_runtime,\n            memory=self.memory\n        )\n        print_text(f'Error analysis for skill \"{train_skill.name}\":\\n')\n        print_text(error_analysis, style='green')\n        if self.memory and update_memory:\n            self.memory.remember(error_analysis, self.skills)\n\n        # 3. IMPROVEMENT PHASE: Improve skills based on analysis\n        print_text(f\"Improve \\\"{train_skill.name}\\\" skill based on analysis ...\")\n        train_skill.improve(\n            error_analysis=error_analysis,\n            runtime=teacher_runtime,\n        )\n        print_text(f'Updated instructions for skill \"{train_skill.name}\":\\n')\n        print_text(train_skill.instructions, style='bold green')\n\n        # 4. RE-APPLY PHASE: Re-apply skills to dataset\n        print_text(f\"Re-apply {train_skill.name} skill to dataset ...\")\n        self.skills[train_skill.name] = train_skill\n        predictions = self.skills.apply(predictions, runtime=runtime, improved_skill=train_skill.name)\n\n    print_text('Train is done!')\n    return ground_truth_signal\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.run","title":"<code>run(dataset=None, runtime=None)</code>","text":"<p>Runs the agent on the specified dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, InternalDataFrame]</code> <p>The dataset to run the agent on.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The name of the runtime to use. Defaults to None, use the default runtime.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The dataset with the agent's predictions.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def run(\n    self,\n    dataset: Optional[Union[Dataset, InternalDataFrame]] = None,\n    runtime: Optional[str] = None\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Runs the agent on the specified dataset.\n\n    Args:\n        dataset (Union[Dataset, InternalDataFrame]): The dataset to run the agent on.\n        runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n    Returns:\n        InternalDataFrame: The dataset with the agent's predictions.\n    \"\"\"\n    if dataset is None:\n        dataset = self.environment.as_dataset()\n    runtime = self.get_runtime(runtime=runtime)\n    predictions = self.skills.apply(dataset, runtime=runtime)\n    return predictions\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and possibly transforms the skills attribute.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[SkillSet, BaseSkill, List[BaseSkill], Dict[str, BaseSkill]]</code> <p>The skills value to validate.</p> required <p>Returns:</p> Name Type Description <code>SkillSet</code> <code>SkillSet</code> <p>The validated set of skills.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator('skills', mode='before')\ndef skills_validator(cls, v) -&gt; SkillSet:\n    \"\"\"\n    Validates and possibly transforms the skills attribute.\n\n    Args:\n        v (Union[SkillSet, BaseSkill, List[BaseSkill], Dict[str, BaseSkill]]): The skills value to validate.\n\n    Returns:\n        SkillSet: The validated set of skills.\n    \"\"\"\n\n    if isinstance(v, SkillSet):\n        return v\n    elif isinstance(v, BaseSkill):\n        return LinearSkillSet(skills={v.name: v})\n    else:\n        return LinearSkillSet(skills=v)\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#adala.datasets.base.BlankDataset","title":"<code>BlankDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Represents an empty dataset with no records.</p> <p>This class can be used in situations where a dataset is required,  but no actual data is available or needed.  All methods return defaults representing an empty state.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>class BlankDataset(Dataset):\n    \"\"\"\n    Represents an empty dataset with no records.\n\n    This class can be used in situations where a dataset is required, \n    but no actual data is available or needed. \n    All methods return defaults representing an empty state.\n    \"\"\"\n\n    def batch_iterator(self, batch_size: int = 100) -&gt; InternalDataFrame:\n        \"\"\"\n        Yields an empty data frame as there are no records in a blank dataset.\n\n        Args:\n            batch_size (int, optional): This argument is ignored for BlankDataset. Defaults to 100.\n\n        Returns:\n            InternalDataFrame: An empty data frame.\n        \"\"\"\n\n        return InternalDataFrame()\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Provides the number of records in the blank dataset (which is always 0).\n\n        Returns:\n            int: Total number of records in the dataset (0 for BlankDataset).\n        \"\"\"\n\n        return 0\n\n    def info(self) -&gt; None:\n        \"\"\"\n        Displays information about the blank dataset.\n        \"\"\"\n\n        print('Blank dataset')\n</code></pre>"},{"location":"datasets/#adala.datasets.base.BlankDataset.__len__","title":"<code>__len__()</code>","text":"<p>Provides the number of records in the blank dataset (which is always 0).</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of records in the dataset (0 for BlankDataset).</p> Source code in <code>adala/datasets/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Provides the number of records in the blank dataset (which is always 0).\n\n    Returns:\n        int: Total number of records in the dataset (0 for BlankDataset).\n    \"\"\"\n\n    return 0\n</code></pre>"},{"location":"datasets/#adala.datasets.base.BlankDataset.batch_iterator","title":"<code>batch_iterator(batch_size=100)</code>","text":"<p>Yields an empty data frame as there are no records in a blank dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>This argument is ignored for BlankDataset. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>An empty data frame.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>def batch_iterator(self, batch_size: int = 100) -&gt; InternalDataFrame:\n    \"\"\"\n    Yields an empty data frame as there are no records in a blank dataset.\n\n    Args:\n        batch_size (int, optional): This argument is ignored for BlankDataset. Defaults to 100.\n\n    Returns:\n        InternalDataFrame: An empty data frame.\n    \"\"\"\n\n    return InternalDataFrame()\n</code></pre>"},{"location":"datasets/#adala.datasets.base.BlankDataset.info","title":"<code>info()</code>","text":"<p>Displays information about the blank dataset.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Displays information about the blank dataset.\n    \"\"\"\n\n    print('Blank dataset')\n</code></pre>"},{"location":"datasets/#adala.datasets.base.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class representing a dataset.</p> <p>Provides methods to interact with and obtain information about datasets.  Concrete implementations should provide functionality for batch iteration,  getting dataset size, and displaying dataset information.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>class Dataset(BaseModel, ABC):\n    \"\"\"\n    Abstract base class representing a dataset.\n\n    Provides methods to interact with and obtain information about datasets. \n    Concrete implementations should provide functionality for batch iteration, \n    getting dataset size, and displaying dataset information.\n    \"\"\"\n\n    @abstractmethod\n    def batch_iterator(self, batch_size: int = 100) -&gt; InternalDataFrame:\n        \"\"\"\n        Yields batches of data records from the dataset.\n\n        Args:\n            batch_size (int, optional): Size of each batch to be yielded. Defaults to 100.\n\n        Returns:\n            InternalDataFrame: A data frame containing a batch of records.\n        \"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Provides the number of records in the dataset.\n\n        Returns:\n            int: Total number of records in the dataset.\n        \"\"\"\n\n    @abstractmethod\n    def info(self) -&gt; None:\n        \"\"\"\n        Displays information about the dataset.\n        \"\"\"\n</code></pre>"},{"location":"datasets/#adala.datasets.base.Dataset.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Provides the number of records in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of records in the dataset.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Provides the number of records in the dataset.\n\n    Returns:\n        int: Total number of records in the dataset.\n    \"\"\"\n</code></pre>"},{"location":"datasets/#adala.datasets.base.Dataset.batch_iterator","title":"<code>batch_iterator(batch_size=100)</code>  <code>abstractmethod</code>","text":"<p>Yields batches of data records from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch to be yielded. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>A data frame containing a batch of records.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>@abstractmethod\ndef batch_iterator(self, batch_size: int = 100) -&gt; InternalDataFrame:\n    \"\"\"\n    Yields batches of data records from the dataset.\n\n    Args:\n        batch_size (int, optional): Size of each batch to be yielded. Defaults to 100.\n\n    Returns:\n        InternalDataFrame: A data frame containing a batch of records.\n    \"\"\"\n</code></pre>"},{"location":"datasets/#adala.datasets.base.Dataset.info","title":"<code>info()</code>  <code>abstractmethod</code>","text":"<p>Displays information about the dataset.</p> Source code in <code>adala/datasets/base.py</code> <pre><code>@abstractmethod\ndef info(self) -&gt; None:\n    \"\"\"\n    Displays information about the dataset.\n    \"\"\"\n</code></pre>"},{"location":"datasets/#adala.datasets.dataframe.DataFrameDataset","title":"<code>DataFrameDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Represents a dataset backed by an internal data frame.</p> <p>Provides methods to interact with and obtain information about the dataset stored as an internal data frame. This class wraps around <code>InternalDataFrame</code> to make it  compatible with the dataset abstraction.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>InternalDataFrame</code> <p>The internal data frame storing the dataset.</p> Source code in <code>adala/datasets/dataframe.py</code> <pre><code>class DataFrameDataset(Dataset):\n    \"\"\"\n    Represents a dataset backed by an internal data frame.\n\n    Provides methods to interact with and obtain information about the dataset stored\n    as an internal data frame. This class wraps around `InternalDataFrame` to make it \n    compatible with the dataset abstraction.\n\n    Attributes:\n        df (InternalDataFrame): The internal data frame storing the dataset.\n    \"\"\"\n\n    df: InternalDataFrame = Field(default_factory=InternalDataFrame)\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Provides the number of records in the dataset.\n\n        Returns:\n            int: Total number of records in the dataset.\n        \"\"\"\n\n        return len(self.df)\n\n    def batch_iterator(self, batch_size: int = 100) -&gt; Iterable[InternalDataFrame]:\n        \"\"\"\n        Yields batches of data records from the dataset.\n\n        Args:\n            batch_size (int, optional): Size of each batch to be yielded. Defaults to 100.\n\n        Yields:\n            Iterable[InternalDataFrame]: An iterator that yields data frames containing batches of records.\n        \"\"\"\n\n        for i in range(0, len(self.df), batch_size):\n            yield self.df.iloc[i:i+batch_size]\n\n    def info(self) -&gt; None:\n        \"\"\"\n        Displays information (statistical description) about the dataset.\n        \"\"\"\n\n        print(self.df.describe())\n</code></pre>"},{"location":"datasets/#adala.datasets.dataframe.DataFrameDataset.__len__","title":"<code>__len__()</code>","text":"<p>Provides the number of records in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of records in the dataset.</p> Source code in <code>adala/datasets/dataframe.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Provides the number of records in the dataset.\n\n    Returns:\n        int: Total number of records in the dataset.\n    \"\"\"\n\n    return len(self.df)\n</code></pre>"},{"location":"datasets/#adala.datasets.dataframe.DataFrameDataset.batch_iterator","title":"<code>batch_iterator(batch_size=100)</code>","text":"<p>Yields batches of data records from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch to be yielded. Defaults to 100.</p> <code>100</code> <p>Yields:</p> Type Description <code>Iterable[InternalDataFrame]</code> <p>Iterable[InternalDataFrame]: An iterator that yields data frames containing batches of records.</p> Source code in <code>adala/datasets/dataframe.py</code> <pre><code>def batch_iterator(self, batch_size: int = 100) -&gt; Iterable[InternalDataFrame]:\n    \"\"\"\n    Yields batches of data records from the dataset.\n\n    Args:\n        batch_size (int, optional): Size of each batch to be yielded. Defaults to 100.\n\n    Yields:\n        Iterable[InternalDataFrame]: An iterator that yields data frames containing batches of records.\n    \"\"\"\n\n    for i in range(0, len(self.df), batch_size):\n        yield self.df.iloc[i:i+batch_size]\n</code></pre>"},{"location":"datasets/#adala.datasets.dataframe.DataFrameDataset.info","title":"<code>info()</code>","text":"<p>Displays information (statistical description) about the dataset.</p> Source code in <code>adala/datasets/dataframe.py</code> <pre><code>def info(self) -&gt; None:\n    \"\"\"\n    Displays information (statistical description) about the dataset.\n    \"\"\"\n\n    print(self.df.describe())\n</code></pre>"},{"location":"environments/","title":"Environments","text":""},{"location":"environments/#adala.environments.base.BasicEnvironment","title":"<code>BasicEnvironment</code>","text":"<p>             Bases: <code>Environment</code></p> <p>A concrete implementation of the Environment abstract base class, assuming the ground truth is provided and comparison is based on exact or fuzzy matching.</p> <p>Attributes:</p> Name Type Description <code>ground_truth_dataset</code> <code>Union[InternalDataFrame, DataFrameDataset]</code> <p>Dataset containing the ground truth data, defaulting to an empty DataFrameDataset.</p> <code>ground_truth_columns</code> <code>Dict[str, str]</code> <p>A dictionary mapping skill names to their corresponding ground truth columns in the dataset.</p> <code>matching_function</code> <code>str</code> <p>The name of the matching function to use, defaults to 'exact'.</p> <code>matching_threshold</code> <code>float</code> <p>The threshold for fuzzy matching, defaults to 0.8.</p> Source code in <code>adala/environments/base.py</code> <pre><code>class BasicEnvironment(Environment):\n    \"\"\"\n    A concrete implementation of the Environment abstract base class,\n    assuming the ground truth is provided and comparison is based on exact or fuzzy matching.\n\n    Attributes:\n        ground_truth_dataset (Union[InternalDataFrame, DataFrameDataset]): Dataset containing\n            the ground truth data, defaulting to an empty DataFrameDataset.\n        ground_truth_columns (Dict[str, str]): A dictionary mapping skill names to their corresponding\n            ground truth columns in the dataset.\n        matching_function (str): The name of the matching function to use, defaults to 'exact'.\n        matching_threshold (float): The threshold for fuzzy matching, defaults to 0.8.\n    \"\"\"    \n    ground_truth_dataset: DataFrameDataset = None\n\n    @field_validator('ground_truth_dataset', mode='before')\n    def _validate_ground_truth_dataset(cls, v):\n        \"\"\"\n        Validate the ground_truth_dataset field to ensure it is converted to DataFrameDataset if needed.\n\n        Args:\n            v: The value to validate.\n\n        Returns:\n            The validated value, possibly converted to DataFrameDataset.\n\n        Raises:\n            ValidationError: If the validation fails.\n        \"\"\"\n\n        if isinstance(v, InternalDataFrame):\n            return DataFrameDataset(df=v)\n        return v\n\n    def request_feedback(self, skills: SkillSet, predictions: InternalDataFrame):\n        \"\"\"\n        In the BasicEnvironment, this method is a placeholder as ground truth is already provided with the input data.\n\n        Args:\n            skill (BaseSkill): The skill being evaluated.\n            predictions (InternalDataFrame): The predictions to be reviewed.\n        \"\"\"\n\n    def get_ground_truth_dataset(self, wait: Optional[float] = None) -&gt; InternalDataFrame:\n        \"\"\"\n        Get the ground truth dataset.\n\n        Returns:\n            InternalDataFrame: The ground truth dataset.\n        \"\"\"\n        return self.ground_truth_dataset.df\n\n    def as_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Return the dataset containing the ground truth data.\n\n        Returns:\n            Dataset: The ground truth dataset as a DataFrameDataset.\n        \"\"\"\n        if self.ground_truth_dataset is not None:\n            return self.ground_truth_dataset\n        return super(BasicEnvironment, self).as_dataset()\n</code></pre>"},{"location":"environments/#adala.environments.base.BasicEnvironment.as_dataset","title":"<code>as_dataset()</code>","text":"<p>Return the dataset containing the ground truth data.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>The ground truth dataset as a DataFrameDataset.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def as_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Return the dataset containing the ground truth data.\n\n    Returns:\n        Dataset: The ground truth dataset as a DataFrameDataset.\n    \"\"\"\n    if self.ground_truth_dataset is not None:\n        return self.ground_truth_dataset\n    return super(BasicEnvironment, self).as_dataset()\n</code></pre>"},{"location":"environments/#adala.environments.base.BasicEnvironment.get_ground_truth_dataset","title":"<code>get_ground_truth_dataset(wait=None)</code>","text":"<p>Get the ground truth dataset.</p> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The ground truth dataset.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_ground_truth_dataset(self, wait: Optional[float] = None) -&gt; InternalDataFrame:\n    \"\"\"\n    Get the ground truth dataset.\n\n    Returns:\n        InternalDataFrame: The ground truth dataset.\n    \"\"\"\n    return self.ground_truth_dataset.df\n</code></pre>"},{"location":"environments/#adala.environments.base.BasicEnvironment.request_feedback","title":"<code>request_feedback(skills, predictions)</code>","text":"<p>In the BasicEnvironment, this method is a placeholder as ground truth is already provided with the input data.</p> <p>Parameters:</p> Name Type Description Default <code>skill</code> <code>BaseSkill</code> <p>The skill being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to be reviewed.</p> required Source code in <code>adala/environments/base.py</code> <pre><code>def request_feedback(self, skills: SkillSet, predictions: InternalDataFrame):\n    \"\"\"\n    In the BasicEnvironment, this method is a placeholder as ground truth is already provided with the input data.\n\n    Args:\n        skill (BaseSkill): The skill being evaluated.\n        predictions (InternalDataFrame): The predictions to be reviewed.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment","title":"<code>Environment</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>An abstract base class that defines the structure and required methods for an environment in which machine learning models operate and are evaluated against ground truth data.</p> <p>Subclasses should implement methods to handle feedback requests, comparison to ground truth, dataset conversion, and state persistence.</p> Source code in <code>adala/environments/base.py</code> <pre><code>class Environment(BaseModel, ABC):\n    \"\"\"\n    An abstract base class that defines the structure and required methods for an environment\n    in which machine learning models operate and are evaluated against ground truth data.\n\n    Subclasses should implement methods to handle feedback requests, comparison to ground truth,\n    dataset conversion, and state persistence.\n    \"\"\"\n    data_stream: Optional[Dataset] = None\n    ground_truth_columns: Optional[Dict[str, str]] = None\n    matching_function: str = 'exact'\n    matching_threshold: float = 0.8\n\n    @field_validator('data_stream', mode='before')\n    def _validate_data_stream(cls, v):\n        \"\"\"\n        Validate the data stream field to ensure it is converted to DataFrameDataset if needed.\n\n        Args:\n            v: The value to validate.\n\n        Returns:\n            The validated value, possibly converted to DataFrameDataset.\n\n        Raises:\n            ValidationError: If the validation fails.\n        \"\"\"\n\n        if isinstance(v, InternalDataFrame):\n            return DataFrameDataset(df=v)\n        return v\n\n    def get_ground_truth_dataset(self, wait: Optional[float] = None) -&gt; InternalDataFrame:\n        \"\"\"\n        Get the ground truth dataset.\n\n        Args:\n            wait (Optional[float], optional): The timeout to wait for ground truth data to be available. Defaults to None.\n\n        Returns:\n            InternalDataFrame: The ground truth dataset.\n        \"\"\"\n\n    @abstractmethod\n    def request_feedback(self, skill_set: SkillSet, predictions: InternalDataFrame):\n        \"\"\"\n        Abstract method to request user feedback on the predictions made by the model.\n\n        Args:\n            skill_set (SkillSet): The set of skills/models whose predictions are being evaluated.\n            predictions (InternalDataFrame): The predictions made by the skills/models.\n        \"\"\"\n\n    def compare_to_ground_truth(\n        self,\n        skill_set: SkillSet,\n        predictions: InternalDataFrame,\n        wait: Optional[float] = None,\n    ) -&gt; Optional[GroundTruthSignal]:\n        \"\"\"\n        Compare the predictions with the ground truth using the specified matching function.\n\n        Args:\n            skill_set (SkillSet): The skill set being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            wait (Optional[float], optional): The timeout to wait for ground truth data to be available. Defaults to None.\n\n        Returns:\n            GroundTruthSignal: The resulting ground truth signal, with matches and errors detailed.\n\n        Raises:\n            NotImplementedError: If the matching_function is unknown.\n        \"\"\"\n\n        ground_truth_match = InternalDataFrame()\n        errors = {}\n        ground_truth_dataset = self.get_ground_truth_dataset(wait=wait)\n        if ground_truth_dataset.empty:\n            return\n\n        for skill_id, skill in skill_set.skills.items():\n            if not self.ground_truth_columns or skill.name not in self.ground_truth_columns:\n                gt_column = skill.name\n            else:\n                gt_column = self.ground_truth_columns[skill.name]\n            gt = ground_truth_dataset[gt_column]\n            pred = predictions[skill.name]\n            # from ground truth dataset, select only the rows that are in the predictions\n            gt, pred = gt.align(pred)\n            nonnull_index = gt.notnull() &amp; pred.notnull()\n            gt = gt[nonnull_index]\n            pred = pred[nonnull_index]\n            # compare ground truth with predictions\n            if self.matching_function == 'exact':\n                gt_pred_match = gt == pred\n            elif self.matching_function == 'fuzzy':\n                gt_pred_match = fuzzy_match(gt, pred, threshold=self.matching_threshold)\n            else:\n                raise NotImplementedError(f'Unknown matching function {self.matching_function}')\n\n            # for values with True, we assume them equal to predictions\n            gt_pred_match[gt == True] = True\n            gt_pred_match[gt == False] = False\n\n            error_index = gt_pred_match[~gt_pred_match].index\n            # concatenate errors - dataframe with two columns: predictions and ground truth\n            errors[skill.name] = InternalDataFrameConcat([pred[error_index], gt[error_index]], axis=1)\n            errors[skill.name].columns = [\"predictions\", gt_column]\n            # concatenate matching columns\n            ground_truth_match = InternalDataFrameConcat([\n                # previous skills' ground truth matches\n                ground_truth_match,\n                # current skill's ground truth match\n                gt_pred_match.rename(skill.name),\n            ], axis=1)\n\n        return GroundTruthSignal(\n            match=ground_truth_match.reindex(predictions.index),\n            errors=errors\n        )\n\n    def as_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Abstract method to convert the environment's state into a dataset.\n\n        Returns:\n            Dataset: A dataset representing the environment's state.\n        \"\"\"\n        return self.data_stream\n\n    def save(self):\n        \"\"\"\n        Save the current state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def restore(self):\n        \"\"\"\n        Restore the state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n        raise NotImplementedError\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.as_dataset","title":"<code>as_dataset()</code>","text":"<p>Abstract method to convert the environment's state into a dataset.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A dataset representing the environment's state.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def as_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Abstract method to convert the environment's state into a dataset.\n\n    Returns:\n        Dataset: A dataset representing the environment's state.\n    \"\"\"\n    return self.data_stream\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.compare_to_ground_truth","title":"<code>compare_to_ground_truth(skill_set, predictions, wait=None)</code>","text":"<p>Compare the predictions with the ground truth using the specified matching function.</p> <p>Parameters:</p> Name Type Description Default <code>skill_set</code> <code>SkillSet</code> <p>The skill set being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>wait</code> <code>Optional[float]</code> <p>The timeout to wait for ground truth data to be available. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GroundTruthSignal</code> <code>Optional[GroundTruthSignal]</code> <p>The resulting ground truth signal, with matches and errors detailed.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the matching_function is unknown.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def compare_to_ground_truth(\n    self,\n    skill_set: SkillSet,\n    predictions: InternalDataFrame,\n    wait: Optional[float] = None,\n) -&gt; Optional[GroundTruthSignal]:\n    \"\"\"\n    Compare the predictions with the ground truth using the specified matching function.\n\n    Args:\n        skill_set (SkillSet): The skill set being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        wait (Optional[float], optional): The timeout to wait for ground truth data to be available. Defaults to None.\n\n    Returns:\n        GroundTruthSignal: The resulting ground truth signal, with matches and errors detailed.\n\n    Raises:\n        NotImplementedError: If the matching_function is unknown.\n    \"\"\"\n\n    ground_truth_match = InternalDataFrame()\n    errors = {}\n    ground_truth_dataset = self.get_ground_truth_dataset(wait=wait)\n    if ground_truth_dataset.empty:\n        return\n\n    for skill_id, skill in skill_set.skills.items():\n        if not self.ground_truth_columns or skill.name not in self.ground_truth_columns:\n            gt_column = skill.name\n        else:\n            gt_column = self.ground_truth_columns[skill.name]\n        gt = ground_truth_dataset[gt_column]\n        pred = predictions[skill.name]\n        # from ground truth dataset, select only the rows that are in the predictions\n        gt, pred = gt.align(pred)\n        nonnull_index = gt.notnull() &amp; pred.notnull()\n        gt = gt[nonnull_index]\n        pred = pred[nonnull_index]\n        # compare ground truth with predictions\n        if self.matching_function == 'exact':\n            gt_pred_match = gt == pred\n        elif self.matching_function == 'fuzzy':\n            gt_pred_match = fuzzy_match(gt, pred, threshold=self.matching_threshold)\n        else:\n            raise NotImplementedError(f'Unknown matching function {self.matching_function}')\n\n        # for values with True, we assume them equal to predictions\n        gt_pred_match[gt == True] = True\n        gt_pred_match[gt == False] = False\n\n        error_index = gt_pred_match[~gt_pred_match].index\n        # concatenate errors - dataframe with two columns: predictions and ground truth\n        errors[skill.name] = InternalDataFrameConcat([pred[error_index], gt[error_index]], axis=1)\n        errors[skill.name].columns = [\"predictions\", gt_column]\n        # concatenate matching columns\n        ground_truth_match = InternalDataFrameConcat([\n            # previous skills' ground truth matches\n            ground_truth_match,\n            # current skill's ground truth match\n            gt_pred_match.rename(skill.name),\n        ], axis=1)\n\n    return GroundTruthSignal(\n        match=ground_truth_match.reindex(predictions.index),\n        errors=errors\n    )\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_ground_truth_dataset","title":"<code>get_ground_truth_dataset(wait=None)</code>","text":"<p>Get the ground truth dataset.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>Optional[float]</code> <p>The timeout to wait for ground truth data to be available. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The ground truth dataset.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_ground_truth_dataset(self, wait: Optional[float] = None) -&gt; InternalDataFrame:\n    \"\"\"\n    Get the ground truth dataset.\n\n    Args:\n        wait (Optional[float], optional): The timeout to wait for ground truth data to be available. Defaults to None.\n\n    Returns:\n        InternalDataFrame: The ground truth dataset.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.request_feedback","title":"<code>request_feedback(skill_set, predictions)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to request user feedback on the predictions made by the model.</p> <p>Parameters:</p> Name Type Description Default <code>skill_set</code> <code>SkillSet</code> <p>The set of skills/models whose predictions are being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skills/models.</p> required Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef request_feedback(self, skill_set: SkillSet, predictions: InternalDataFrame):\n    \"\"\"\n    Abstract method to request user feedback on the predictions made by the model.\n\n    Args:\n        skill_set (SkillSet): The set of skills/models whose predictions are being evaluated.\n        predictions (InternalDataFrame): The predictions made by the skills/models.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.restore","title":"<code>restore()</code>","text":"<p>Restore the state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def restore(self):\n    \"\"\"\n    Restore the state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.save","title":"<code>save()</code>","text":"<p>Save the current state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def save(self):\n    \"\"\"\n    Save the current state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"environments/#adala.environments.base.GroundTruthSignal","title":"<code>GroundTruthSignal</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A model that represents the comparison between predictions and ground truth data, potentially holding information about matching results and errors per skill.</p> <p>Attributes:</p> Name Type Description <code>match</code> <code>InternalDataFrame</code> <p>A DataFrame indicating the correctness of predictions.                        Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.                        Columns are named after the skill names.                        Indices correspond to prediction indices.                        Example:                            <code>| index | skill_1 | skill_2 | skill_3 |                             |-------|---------|---------|---------|                             | 0     | True    | True    | False   |                             | 1     | False   | False   | False   |                             | 2     | True    | True    | True    |</code></p> <code>errors</code> <code>Optional[Dict[str, InternalDataFrame]]</code> <p>A dictionary mapping skill names to DataFrames containing the errors between predictions and ground truth. Default is None. Each DataFrame has two columns [\"predictions\", \"user-defined ground truth name\"]. User defined ground truth name is taken from Environment Example:</p> <pre><code>    {\n        \"skill_1\": InternalDataFrame({\n            \"predictions\": ['a', 'b', 'c'],\n            \"my_gr_truth\": ['a', 'a', 'c']\n        }, index=[0, 1, 2])\n    }\n</code></pre> Source code in <code>adala/environments/base.py</code> <pre><code>class GroundTruthSignal(BaseModel):\n    \"\"\"\n    A model that represents the comparison between predictions and ground truth data,\n    potentially holding information about matching results and errors per skill.\n\n    Attributes:\n        match (InternalDataFrame): A DataFrame indicating the correctness of predictions.\n                                   Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.\n                                   Columns are named after the skill names.\n                                   Indices correspond to prediction indices.\n                                   Example:\n                                       ```\n                                        | index | skill_1 | skill_2 | skill_3 |\n                                        |-------|---------|---------|---------|\n                                        | 0     | True    | True    | False   |\n                                        | 1     | False   | False   | False   |\n                                        | 2     | True    | True    | True    |\n                                        ```\n        errors (Optional[Dict[str, InternalDataFrame]]): A dictionary mapping skill names to DataFrames\n            containing the errors between predictions and ground truth. Default is None.\n            Each DataFrame has two columns [\"predictions\", \"user-defined ground truth name\"].\n            User defined ground truth name is taken from Environment\n            Example:\n            ```json\n                {\n                    \"skill_1\": InternalDataFrame({\n                        \"predictions\": ['a', 'b', 'c'],\n                        \"my_gr_truth\": ['a', 'a', 'c']\n                    }, index=[0, 1, 2])\n                }\n            ```\n    \"\"\"\n\n    match: InternalDataFrame = Field(default_factory=InternalDataFrame)\n    errors: Optional[Dict[str, InternalDataFrame]] = None\n\n    def get_accuracy(self) -&gt; InternalSeries:\n        \"\"\"\n        Calculate the accuracy of predictions as the mean of matches.\n\n        Returns:\n            InternalSeries: A series representing the accuracy of predictions.\n\n        Examples:\n\n\n        \"\"\"\n\n        return self.match.mean()\n\n    def get_errors(self, skill_name: str) -&gt; InternalDataFrame:\n        \"\"\"\n        Retrieve the errors associated with a particular skill.\n\n        Args:\n            skill_name (str): The name of the skill to retrieve errors for.\n\n        Returns:\n            InternalDataFrame: A DataFrame with two columns [\"predictions\", \"ground_truth name\"]\n            representing the errors.\n\n        Raises:\n            AssertionError: If the error DataFrame does not have exactly two columns.\n        \"\"\"\n\n        errors = self.errors[skill_name]\n        assert len(errors.columns) == 2  # [\"predictions\", \"ground_truth name\"]\n        return errors\n\n    def __rich__(self):\n        text = '[bold blue]Ground Truth Signal:[/bold blue]\\n\\n'\n        text += f'\\n[bold]Match[/bold]\\n{self.match}'\n        if self.errors is not None:\n            for skill_name, errors in self.errors.items():\n                text += f'\\n[bold]Errors for {skill_name}[/bold]\\n{errors}'\n        return text\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.GroundTruthSignal.get_accuracy","title":"<code>get_accuracy()</code>","text":"<p>Calculate the accuracy of predictions as the mean of matches.</p> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>A series representing the accuracy of predictions.</p> <p>Examples:</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_accuracy(self) -&gt; InternalSeries:\n    \"\"\"\n    Calculate the accuracy of predictions as the mean of matches.\n\n    Returns:\n        InternalSeries: A series representing the accuracy of predictions.\n\n    Examples:\n\n\n    \"\"\"\n\n    return self.match.mean()\n</code></pre>"},{"location":"environments/#adala.environments.base.GroundTruthSignal.get_errors","title":"<code>get_errors(skill_name)</code>","text":"<p>Retrieve the errors associated with a particular skill.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>The name of the skill to retrieve errors for.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>A DataFrame with two columns [\"predictions\", \"ground_truth name\"]</p> <code>InternalDataFrame</code> <p>representing the errors.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the error DataFrame does not have exactly two columns.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_errors(self, skill_name: str) -&gt; InternalDataFrame:\n    \"\"\"\n    Retrieve the errors associated with a particular skill.\n\n    Args:\n        skill_name (str): The name of the skill to retrieve errors for.\n\n    Returns:\n        InternalDataFrame: A DataFrame with two columns [\"predictions\", \"ground_truth name\"]\n        representing the errors.\n\n    Raises:\n        AssertionError: If the error DataFrame does not have exactly two columns.\n    \"\"\"\n\n    errors = self.errors[skill_name]\n    assert len(errors.columns) == 2  # [\"predictions\", \"ground_truth name\"]\n    return errors\n</code></pre>"},{"location":"memories/","title":"Memories","text":""},{"location":"memories/#adala.memories.base.Memory","title":"<code>Memory</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class for long-term memories. Long-term memories are used to store acquired knowledge and can be shared between agents.</p> Source code in <code>adala/memories/base.py</code> <pre><code>class Memory(BaseModel, ABC):\n\n    \"\"\"\n    Base class for long-term memories.\n    Long-term memories are used to store acquired knowledge and can be shared between agents.\n    \"\"\"\n\n    @abstractmethod\n    def remember(self, observation: str, experience: Any):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n\n    @abstractmethod\n    def retrieve(self, observation: str) -&gt; Any:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n        \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember","title":"<code>remember(observation, experience)</code>  <code>abstractmethod</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef remember(self, observation: str, experience: Any):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve","title":"<code>retrieve(observation)</code>  <code>abstractmethod</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, observation: str) -&gt; Any:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory","title":"<code>FileMemory</code>","text":"<p>             Bases: <code>Memory</code></p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>class FileMemory(Memory):\n\n    filepath: str\n\n    def remember(self, observation: str, experience: Any):\n        \"\"\"\n        Serialize experience in JSON and append to file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        memory[observation] = experience\n        with open(self.filepath, 'w') as f:\n            json.dump(memory, f, indent=2)\n\n    def retrieve(self, observation: str) -&gt; Any:\n        \"\"\"\n        Retrieve experience from file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        return memory[observation]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.remember","title":"<code>remember(observation, experience)</code>","text":"<p>Serialize experience in JSON and append to file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def remember(self, observation: str, experience: Any):\n    \"\"\"\n    Serialize experience in JSON and append to file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    memory[observation] = experience\n    with open(self.filepath, 'w') as f:\n        json.dump(memory, f, indent=2)\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.retrieve","title":"<code>retrieve(observation)</code>","text":"<p>Retrieve experience from file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def retrieve(self, observation: str) -&gt; Any:\n    \"\"\"\n    Retrieve experience from file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    return memory[observation]\n</code></pre>"},{"location":"runtimes/","title":"Runtimes","text":""},{"location":"runtimes/#adala.runtimes.base.CodeRuntime","title":"<code>CodeRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Base class representing a runtime designed for executing code.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class CodeRuntime(Runtime):\n    \"\"\"Base class representing a runtime designed for executing code.\"\"\"\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime","title":"<code>LLMRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Class representing an LLM runtime environment.</p> <p>Attributes:</p> Name Type Description <code>llm_runtime_type</code> <code>LLMRuntimeModelType</code> <p>Type of the LLM runtime. Defaults to OpenAI.</p> <code>llm_params</code> <code>Dict[str, str]</code> <p>Parameters for the LLM runtime. Defaults to a basic GPT-3.5 configuration.</p> <code>_llm</code> <p>Internal instance for the LLM model. Initialized in <code>init_runtime</code>.</p> <code>_program</code> <p>Program instance used for guidance. Initialized in <code>init_runtime</code>.</p> <code>_llm_template</code> <code>str</code> <p>Template string for LLM guidance.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class LLMRuntime(Runtime):\n    \"\"\"\n    Class representing an LLM runtime environment.\n\n    Attributes:\n        llm_runtime_type (LLMRuntimeModelType): Type of the LLM runtime. Defaults to OpenAI.\n        llm_params (Dict[str, str]): Parameters for the LLM runtime. Defaults to a basic GPT-3.5 configuration.\n\n        _llm: Internal instance for the LLM model. Initialized in `init_runtime`.\n        _program: Program instance used for guidance. Initialized in `init_runtime`.\n        _llm_template (str): Template string for LLM guidance.\n    \"\"\"\n    llm_runtime_type: LLMRuntimeType = LLMRuntimeType.STUDENT\n    llm_runtime_model_type: LLMRuntimeModelType = LLMRuntimeModelType.OpenAI\n    llm_params: Dict[str, str] = {\n        'model': 'gpt-3.5-turbo-instruct',\n        # 'max_tokens': 10,\n        # 'temperature': 0,\n    }\n    _llm = None\n    _program = None\n    # do not override this template\n    _llm_template: str = '''\\\n{{&gt;instructions_program}}\n\n{{&gt;input_program}}\n{{&gt;output_program}}'''\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def _create_program(self):\n        # create an LLM instance\n        if self.llm_runtime_model_type.value == LLMRuntimeModelType.OpenAI.value:\n            self._llm = guidance.llms.OpenAI(**self.llm_params)\n        elif self.llm_runtime_model_type.value == LLMRuntimeModelType.Transformers.value:\n            self._llm = guidance.llms.Transformers(**self.llm_params)\n        else:\n            raise NotImplementedError(f'LLM runtime type {self.llm_runtime_model_type} is not implemented.')\n        self._program = guidance(self._llm_template, llm=self._llm, silent=not self.verbose)\n\n    def init_runtime(self) -&gt; 'LLMRuntime':\n        \"\"\"Initializes the LLM runtime environment.\n\n        Creates an LLM instance based on the runtime type and parameters.\n\n        Returns:\n            LLMRuntime: Initialized runtime instance.\n        \"\"\"\n        self._create_program()\n        return self\n\n    def get_outputs(self, output_template: Optional[str] = None) -&gt; List[str]:\n        \"\"\"Extracts output fields from the output template.\n\n        Args:\n            output_template (str): The template string to extract output fields from.\n\n        Returns:\n            List[str]: List of extracted output fields.\n        \"\"\"\n        # search for all occurrences of {{...'output'...}}\n        # TODO: this is a very naive regex implementation - likely to fail in many cases\n        if output_template is None:\n            return []\n        outputs = re.findall(r'\\'(.*?)\\'', output_template)\n        return outputs\n\n    def _process_record(\n        self,\n        record,\n        program,\n        extra_fields,\n        outputs=None\n    ) -&gt; Dict[str, Any]:\n\n        \"\"\"Processes a single record using the guidance program.\n\n        Args:\n            record (dict or InternalDataFrame): The record to be processed.\n            program (callable): The guidance program for processing.\n            extra_fields (dict, optional): Additional fields to include in the processed record.\n            outputs (list of str, optional): Specific output fields to extract from the result.\n\n        Returns:\n            dict: Processed output for the record.\n        \"\"\"\n\n        if not isinstance(record, dict):\n            record = record.to_dict()\n        else:\n            record = record.copy()\n        verified_input = record\n        # exclude guidance parameter from input\n        if 'text' in verified_input:\n            verified_input['text_'] = verified_input['text']\n            del verified_input['text']\n        verified_input.update(extra_fields)\n        if self.verbose:\n            print_text(str(verified_input))\n        result = program(\n            silent=not self.verbose,\n            **verified_input\n        )\n        if not outputs:\n            verified_output = {'': str(result)}\n        else:\n            verified_output = {field: result[field] for field in outputs}\n\n        return verified_output\n\n    def get_input_program(self, input_template) -&gt; Callable:\n        \"\"\"Generates an input program from the provided template.\n\n        Args:\n            input_template (str): Template to generate the input program.\n\n        Returns:\n            callable: The generated input program.\n        \"\"\"\n\n        # fix input template in case \"text\" is presented there - there might be other paramater names as well...\n        fixed_input_template = input_template\n        if '{{text}}' in fixed_input_template:\n            fixed_input_template = fixed_input_template.replace('{{text}}', '{{text_}}')\n        input_program = guidance(fixed_input_template, llm=self._llm, silent=not self.verbose)\n        return input_program\n\n    def get_output_program(self, output_template) -&gt; Callable:\n        \"\"\"Generates an output program from the provided template.\n\n        Args:\n            output_template (str): Template to generate the output program.\n\n        Returns:\n            callable: The generated output program.\n        \"\"\"\n\n        output_program = guidance(output_template, llm=self._llm)\n        return output_program\n\n    def get_instructions_program(self, instructions) -&gt; Callable:\n        \"\"\"Generates an instructions program from the provided template.\n\n        Args:\n            instructions (str): The instructions to generate the program.\n\n        Returns:\n            callable: The generated instructions program.\n        \"\"\"\n\n        instructions_program = guidance(instructions, llm=self._llm)\n        return instructions_program\n\n    def _prepare_program_and_params(self, input_template, output_template, instructions, extra_fields):\n        extra_fields = extra_fields or {}\n        extra_fields = extra_fields.copy()\n        # if only one program template is provided, use it as a program\n        if output_template is None and instructions is None:\n            program = self.get_input_program(input_template)\n        else:\n            program = self._program\n            extra_fields.update({\n                'input_program': self.get_input_program(input_template),\n                'output_program': self.get_output_program(output_template),\n                'instructions_program': self.get_instructions_program(instructions),\n            })\n        return program, extra_fields\n\n    def process_record(\n        self,\n        record: Dict[str, Any],\n        input_template: str,\n        output_template: Optional[str] = None,\n        instructions: Optional[str] = None,\n        extra_fields: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Processes a record using the provided templates and instructions.\n\n        Args:\n            record (Dict[str, Any]): The record data to be processed.\n            input_template (str): Template for input processing.\n            output_template (str): Template for output processing.\n            instructions (str): Instructions for guidance.\n            extra_fields (Dict[str, Any], optional): Additional fields to include during processing.\n\n        Returns:\n            Dict[str, Any]: The processed record.\n        \"\"\"\n        outputs = self.get_outputs(output_template)\n        program, extra_fields = self._prepare_program_and_params(input_template, output_template, instructions, extra_fields)\n        output = self._process_record(\n            record=record,\n            program=program,\n            outputs=outputs,\n            extra_fields=extra_fields\n        )\n        return output\n\n    def process_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        output_template: Optional[str] = None,\n        instructions: Optional[str] = None,\n        extra_fields: Optional[Dict[str, Any]] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"Processes a batch of records using the provided templates and instructions.\n\n        Args:\n            batch (InternalDataFrame): The batch of records to be processed.\n            input_template (str): Template for input processing.\n            output_template (str): Template for output processing.\n            instructions (str): Instructions for guidance.\n            extra_fields (Dict[str, Any], optional): Additional fields to include during batch processing.\n\n        Returns:\n            InternalDataFrame: The processed batch of records.\n        \"\"\"\n\n        outputs = self.get_outputs(output_template)\n        program, extra_fields = self._prepare_program_and_params(input_template, output_template, instructions, extra_fields)\n        output = batch.progress_apply(\n            self._process_record,\n            axis=1,\n            result_type='expand',\n            program=program,\n            outputs=outputs,\n            extra_fields=extra_fields\n        )\n        return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.get_input_program","title":"<code>get_input_program(input_template)</code>","text":"<p>Generates an input program from the provided template.</p> <p>Parameters:</p> Name Type Description Default <code>input_template</code> <code>str</code> <p>Template to generate the input program.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>The generated input program.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def get_input_program(self, input_template) -&gt; Callable:\n    \"\"\"Generates an input program from the provided template.\n\n    Args:\n        input_template (str): Template to generate the input program.\n\n    Returns:\n        callable: The generated input program.\n    \"\"\"\n\n    # fix input template in case \"text\" is presented there - there might be other paramater names as well...\n    fixed_input_template = input_template\n    if '{{text}}' in fixed_input_template:\n        fixed_input_template = fixed_input_template.replace('{{text}}', '{{text_}}')\n    input_program = guidance(fixed_input_template, llm=self._llm, silent=not self.verbose)\n    return input_program\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.get_instructions_program","title":"<code>get_instructions_program(instructions)</code>","text":"<p>Generates an instructions program from the provided template.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions to generate the program.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>The generated instructions program.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def get_instructions_program(self, instructions) -&gt; Callable:\n    \"\"\"Generates an instructions program from the provided template.\n\n    Args:\n        instructions (str): The instructions to generate the program.\n\n    Returns:\n        callable: The generated instructions program.\n    \"\"\"\n\n    instructions_program = guidance(instructions, llm=self._llm)\n    return instructions_program\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.get_output_program","title":"<code>get_output_program(output_template)</code>","text":"<p>Generates an output program from the provided template.</p> <p>Parameters:</p> Name Type Description Default <code>output_template</code> <code>str</code> <p>Template to generate the output program.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>The generated output program.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def get_output_program(self, output_template) -&gt; Callable:\n    \"\"\"Generates an output program from the provided template.\n\n    Args:\n        output_template (str): Template to generate the output program.\n\n    Returns:\n        callable: The generated output program.\n    \"\"\"\n\n    output_program = guidance(output_template, llm=self._llm)\n    return output_program\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.get_outputs","title":"<code>get_outputs(output_template=None)</code>","text":"<p>Extracts output fields from the output template.</p> <p>Parameters:</p> Name Type Description Default <code>output_template</code> <code>str</code> <p>The template string to extract output fields from.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of extracted output fields.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def get_outputs(self, output_template: Optional[str] = None) -&gt; List[str]:\n    \"\"\"Extracts output fields from the output template.\n\n    Args:\n        output_template (str): The template string to extract output fields from.\n\n    Returns:\n        List[str]: List of extracted output fields.\n    \"\"\"\n    # search for all occurrences of {{...'output'...}}\n    # TODO: this is a very naive regex implementation - likely to fail in many cases\n    if output_template is None:\n        return []\n    outputs = re.findall(r'\\'(.*?)\\'', output_template)\n    return outputs\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.init_runtime","title":"<code>init_runtime()</code>","text":"<p>Initializes the LLM runtime environment.</p> <p>Creates an LLM instance based on the runtime type and parameters.</p> <p>Returns:</p> Name Type Description <code>LLMRuntime</code> <code>LLMRuntime</code> <p>Initialized runtime instance.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def init_runtime(self) -&gt; 'LLMRuntime':\n    \"\"\"Initializes the LLM runtime environment.\n\n    Creates an LLM instance based on the runtime type and parameters.\n\n    Returns:\n        LLMRuntime: Initialized runtime instance.\n    \"\"\"\n    self._create_program()\n    return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.process_batch","title":"<code>process_batch(batch, input_template, output_template=None, instructions=None, extra_fields=None)</code>","text":"<p>Processes a batch of records using the provided templates and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>InternalDataFrame</code> <p>The batch of records to be processed.</p> required <code>input_template</code> <code>str</code> <p>Template for input processing.</p> required <code>output_template</code> <code>str</code> <p>Template for output processing.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for guidance.</p> <code>None</code> <code>extra_fields</code> <code>Dict[str, Any]</code> <p>Additional fields to include during batch processing.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch of records.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def process_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    output_template: Optional[str] = None,\n    instructions: Optional[str] = None,\n    extra_fields: Optional[Dict[str, Any]] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"Processes a batch of records using the provided templates and instructions.\n\n    Args:\n        batch (InternalDataFrame): The batch of records to be processed.\n        input_template (str): Template for input processing.\n        output_template (str): Template for output processing.\n        instructions (str): Instructions for guidance.\n        extra_fields (Dict[str, Any], optional): Additional fields to include during batch processing.\n\n    Returns:\n        InternalDataFrame: The processed batch of records.\n    \"\"\"\n\n    outputs = self.get_outputs(output_template)\n    program, extra_fields = self._prepare_program_and_params(input_template, output_template, instructions, extra_fields)\n    output = batch.progress_apply(\n        self._process_record,\n        axis=1,\n        result_type='expand',\n        program=program,\n        outputs=outputs,\n        extra_fields=extra_fields\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntime.process_record","title":"<code>process_record(record, input_template, output_template=None, instructions=None, extra_fields=None)</code>","text":"<p>Processes a record using the provided templates and instructions.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, Any]</code> <p>The record data to be processed.</p> required <code>input_template</code> <code>str</code> <p>Template for input processing.</p> required <code>output_template</code> <code>str</code> <p>Template for output processing.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for guidance.</p> <code>None</code> <code>extra_fields</code> <code>Dict[str, Any]</code> <p>Additional fields to include during processing.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The processed record.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def process_record(\n    self,\n    record: Dict[str, Any],\n    input_template: str,\n    output_template: Optional[str] = None,\n    instructions: Optional[str] = None,\n    extra_fields: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Processes a record using the provided templates and instructions.\n\n    Args:\n        record (Dict[str, Any]): The record data to be processed.\n        input_template (str): Template for input processing.\n        output_template (str): Template for output processing.\n        instructions (str): Instructions for guidance.\n        extra_fields (Dict[str, Any], optional): Additional fields to include during processing.\n\n    Returns:\n        Dict[str, Any]: The processed record.\n    \"\"\"\n    outputs = self.get_outputs(output_template)\n    program, extra_fields = self._prepare_program_and_params(input_template, output_template, instructions, extra_fields)\n    output = self._process_record(\n        record=record,\n        program=program,\n        outputs=outputs,\n        extra_fields=extra_fields\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.LLMRuntimeModelType","title":"<code>LLMRuntimeModelType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enumeration for LLM runtime model types.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class LLMRuntimeModelType(enum.Enum):\n    \"\"\"Enumeration for LLM runtime model types.\"\"\"    \n    OpenAI = 'OpenAI'\n    Transformers = 'Transformers'\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime","title":"<code>Runtime</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class representing a generic runtime environment.</p> <p>Attributes:</p> Name Type Description <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class Runtime(BaseModel, ABC):\n    \"\"\"\n    Base class representing a generic runtime environment.\n\n    Attributes:\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n    \"\"\"\n    verbose: bool = False\n\n    @model_validator(mode='after')\n    def init_runtime(self) -&gt; 'Runtime':\n        \"\"\"Initializes the runtime.\n\n        This method should be used to validate and potentially initialize the runtime instance.\n\n        Returns:\n            Runtime: The initialized runtime instance.\n        \"\"\"  \n        return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.init_runtime","title":"<code>init_runtime()</code>","text":"<p>Initializes the runtime.</p> <p>This method should be used to validate and potentially initialize the runtime instance.</p> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The initialized runtime instance.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@model_validator(mode='after')\ndef init_runtime(self) -&gt; 'Runtime':\n    \"\"\"Initializes the runtime.\n\n    This method should be used to validate and potentially initialize the runtime instance.\n\n    Returns:\n        Runtime: The initialized runtime instance.\n    \"\"\"  \n    return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes.openai.OpenAIRuntime","title":"<code>OpenAIRuntime</code>","text":"<p>             Bases: <code>LLMRuntime</code></p> <p>Runtime class specifically designed for OpenAI models.</p> <p>This class is tailored to use OpenAI models, particularly GPT models. It inherits from the <code>LLMRuntime</code> class and thus can utilize its functionalities but specializes  for the OpenAI ecosystem.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key required to access OpenAI's API.</p> <code>gpt_model_name</code> <code>str</code> <p>Name of the GPT model. Defaults to 'gpt-3.5-turbo-instruct'.</p> <code>temperature</code> <code>float</code> <p>Sampling temperature for the GPT model's output.                   A higher value makes output more random, while a lower value makes it more deterministic.                  Defaults to 0.0.</p> Source code in <code>adala/runtimes/openai.py</code> <pre><code>class OpenAIRuntime(LLMRuntime):\n    \"\"\"Runtime class specifically designed for OpenAI models.\n\n    This class is tailored to use OpenAI models, particularly GPT models.\n    It inherits from the `LLMRuntime` class and thus can utilize its functionalities but specializes \n    for the OpenAI ecosystem.\n\n    Attributes:\n        api_key (str): The API key required to access OpenAI's API.\n        gpt_model_name (str): Name of the GPT model. Defaults to 'gpt-3.5-turbo-instruct'.\n        temperature (float): Sampling temperature for the GPT model's output. \n                             A higher value makes output more random, while a lower value makes it more deterministic.\n                             Defaults to 0.0.\n    \"\"\"\n\n    api_key: Optional[str] = None\n    gpt_model_name: Optional[str] = Field(default='gpt-3.5-turbo-instruct', alias='model')\n    temperature: Optional[float] = 0.0\n\n    def _check_api_key(self):\n        if self.api_key:\n            return\n        self.api_key = os.getenv('OPENAI_API_KEY')\n        if not self.api_key:\n            print_error(\n                'OpenAI API key is not provided. Please set the OPENAI_API_KEY environment variable:\\n\\n'\n                'export OPENAI_API_KEY=your-openai-api-key\\n\\n'\n                'or set the `api_key` attribute of the `OpenAIRuntime` python class:\\n\\n'\n                f'{self.__class__.__name__}(..., api_key=\"your-openai-api-key\")\\n\\n'\n                f'Read more about OpenAI API keys at https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key')\n            raise ValueError('OpenAI API key is not provided.')\n\n    def _check_model_availability(self):\n        models = openai.Model.list(api_key=self.api_key)\n        models = set(model['id'] for model in models['data'])\n        if self.gpt_model_name not in models:\n            print_error(\n                f'Requested model \"{self.gpt_model_name}\" is not available in your OpenAI account. '\n                f'Available models are: {models}\\n\\n'\n                f'Try to change the runtime settings for {self.__class__.__name__}, for example:\\n\\n'\n                f'{self.__class__.__name__}(..., model=\"gpt-3.5-turbo\")\\n\\n'\n            )\n            raise ValueError(f'Requested model {self.gpt_model_name} is not available in your OpenAI account.')\n\n    def init_runtime(self):\n        self._check_api_key()\n        self._check_model_availability()\n\n        student_models = {'gpt-3.5-turbo-instruct', 'text-davinci-003'}\n        teacher_models = {'gpt-4', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-4-1106-preview', 'gpt-4-vision-preview'}\n\n        if self.gpt_model_name in student_models:\n            self.llm_runtime_type = LLMRuntimeType.STUDENT\n        elif self.gpt_model_name in teacher_models:\n            self.llm_runtime_type = LLMRuntimeType.TEACHER\n        else:\n            raise NotImplementedError(f'Not supported model: {self.gpt_model_name}.')\n\n        self.llm_runtime_model_type = LLMRuntimeModelType.OpenAI\n        self.llm_params = {\n            'model': self.gpt_model_name,\n            'temperature': self.temperature,\n            'api_key': self.api_key\n        }\n        self._create_program()\n        return self\n</code></pre>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#adala.skills.base.BaseSkill","title":"<code>BaseSkill</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A foundational abstract class representing a skill. This class sets the foundation  for all skills and provides common attributes and methods for skill-based operations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>description</code> <code>str</code> <p>Description of the skill.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>input_data_field</code> <code>str</code> <p>Name of the input data field.</p> <code>prediction_field</code> <code>str</code> <p>Name of the prediction field to be used for the output data.</p> Source code in <code>adala/skills/base.py</code> <pre><code>class BaseSkill(BaseModel, ABC):\n    \"\"\"\n    A foundational abstract class representing a skill. This class sets the foundation \n    for all skills and provides common attributes and methods for skill-based operations.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        description (str): Description of the skill.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        input_data_field (str): Name of the input data field.\n        prediction_field (str): Name of the prediction field to be used for the output data.\n    \"\"\"\n    name: str = Field(\n        title='Skill name',\n        description='Unique name of the skill',\n        default='',\n        examples=['labeling', 'classification', 'text-generation']\n    )\n    instructions: str = Field(\n        title='Skill instructions',\n        description='Instructs agent what to do with the input data. '\n                    'Can use templating to refer to input fields.',\n        default='',\n        examples=['Label the input text with the following labels: {{labels}}']\n    )\n    description: Optional[str] = Field(\n        default='',\n        title='Skill description',\n        description='Description of the skill. Can be used to retrieve skill from the library.',\n        examples=['The skill to perform sentiment analysis on the input text.']\n    )\n    input_template: Optional[str] = Field(\n        title='Input template',\n        description='Template for the input data. '\n                    'Can use templating to refer to input parameters and perform data transformations.',\n        default=\"Input: {{{{{input}}}}}\",\n        examples=[\"Text: {{{{{input}}}}}, Date: {{{{date_column}}}}, Sentiment: {{{{gen 'sentiment'}}}}\"]\n    )\n    # TODO: skill can have multiple input fields\n    input_data_field: Optional[str] = Field(\n        title='Input data field',\n        description='Input data field name that will be used to match input data.',\n        examples=['text'],\n        # TODO: either make it required, or `input_template` required\n        default=None\n    )\n    output_template: Optional[str] = Field(\n        title='Output template',\n        description='Template for the output data. '\n                    'Can use templating to refer to input parameters and perform data transformations',\n        default=\"Output: {{gen 'predictions'}}\",\n        examples=[\"Output: {{select 'predictions' options=labels logprobs='score'}}\"]\n    )\n    prediction_field: Optional[str] = Field(\n        title='Prediction field',\n        description='Prediction field name that will be used to match ground truth labels.'\n                    'Should match at least one output field in `output_template`, e.g. \\'predictions\\'',\n        examples=['predictions'],\n        default='predictions'\n    )\n\n    @model_validator(mode='after')\n    def validate_inputs(self) -&gt; 'BaseSkill':\n        \"\"\"\n        Validates the input_template, updating it if necessary.\n\n        Returns:\n            BaseSkill: Updated instance of the BaseSkill class.\n        \"\"\"\n        if '{{{{{input}}}}}' in self.input_template:\n            if self.input_data_field is None:\n                print_error(f'You provided skill \"{self.name}\" with input template:\\n\\n'\n                            f'{self.__class__.__name__}.input_template = \"{self.input_template}\"\\n\\n'\n                            'that contains \"{{{{{input}}}}}\" placeholder. (yes... 5 curly braces!) \\n\\n'\n                            'In this case, you have to provide skill with `skill.input_data_field` to match the input data.'\n                            f'\\nFor example, if your input data stored in `\"text\"` column, '\n                            f'you can set\\n\\nskill = {self.__class__.__name__}(..., input_data_field=\"text\")')\n                raise ValueError(f'`input_data_field` is not provided for skill {self.name}')\n            self.input_template = self.input_template.format(input=self.input_data_field)\n        return self\n\n    def __call__(self, input: InternalDataFrame, runtime: Runtime, dataset: Dataset) -&gt; InternalDataFrame:\n        \"\"\"Calls the runtime to process a batch of inputs. Input and\n        output shapes can be varying, and it should also take care of\n        data types validation\n\n        Args:\n            input (InternalDataFrame): Input data in the form of an InternalDataFrame.\n            runtime (Runtime): The runtime instance to be used for processing.\n            dataset (Dataset): The dataset containing the data to be processed.\n\n        Returns:\n            InternalDataFrame: Concatenated dataframe with the original input and the predictions from the runtime.\n\n        \"\"\"\n\n        # get user defined dataset input fields\n\n        runtime_predictions = runtime.process_batch(\n            batch=input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions=self.instructions,\n            extra_fields=self._get_extra_fields()\n        )\n        runtime_predictions.rename(columns={self.prediction_field: self.name}, inplace=True)\n        output = input.copy()\n        output[runtime_predictions.columns] = runtime_predictions[runtime_predictions.columns]\n        return output\n\n    def _get_extra_fields(self):\n        \"\"\"\n        Retrieves fields that are not categorized as system fields.\n\n        Returns:\n            dict: A dictionary containing fields that are not system fields.\n        \"\"\"\n\n        # TODO: more robust way to exclude system fields\n        system_fields = {\n            'name', 'description', 'input_template', 'output_template', 'instructions',\n            'input_data_field', 'prediction_field'}\n        extra_fields = self.model_dump(exclude=system_fields)\n        return extra_fields\n\n    @abstractmethod\n    def apply(\n        self, dataset: Dataset,\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataset and returns the results.\n\n        Args:\n            dataset (Dataset): The dataset on which the skill is to be applied.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            ShortTermMemory: The updated experience after applying the skill.\n        \"\"\"        \n\n    @abstractmethod\n    def analyze(\n        self,\n        predictions: InternalDataFrame,\n        errors: InternalDataFrame,\n        student_runtime: Runtime,\n        teacher_runtime: Optional[Runtime] = None,\n        memory: Optional[Memory] = None,\n    ) -&gt; str:\n        \"\"\"\n        Analyzes the results to derive new experiences.\n        It gets provided skill predictions in the format:\n\n        ```markdown\n        | input | skill_1 | skill_2 | skill_3 |\n        |-------|---------|---------|---------|\n        | text1 | label11 | label21 | label31 |\n        | text2 | label12 | label22 | label32 |\n        | ...   | ...     | ...     | ...     |\n        ```\n\n        and the errors for a specific skill to analyze in the format:\n\n        ```markdown\n        | prediction | ground_truth |\n        |------------|--------------|\n        | label11    | label12      |\n        | ...        | ...          |\n        ```\n\n        and returns the string that contains the error analysis report.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            errors (InternalDataFrame): The errors made by the skill.\n            student_runtime (Runtime): The runtime instance used to get predictions.\n            teacher_runtime (Optional[Runtime]): The runtime instance to be used for analysing the errors.\n            memory (Optional[Memory]): The memory instance to be used for processing.\n\n        Returns:\n            str: The error analysis report.\n        \"\"\"\n\n    @abstractmethod\n    def improve(\n        self,\n        error_analysis: str,\n        runtime: Runtime\n    ):\n        \"\"\"\n        Refines the LLM skill based on its recent experiences and updates the skill's instructions.\n\n        Args:\n            error_analysis (str): The error analysis report.\n            runtime (Runtime): The runtime instance to be used for processing.\n        \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.base.BaseSkill.__call__","title":"<code>__call__(input, runtime, dataset)</code>","text":"<p>Calls the runtime to process a batch of inputs. Input and output shapes can be varying, and it should also take care of data types validation</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>Input data in the form of an InternalDataFrame.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset containing the data to be processed.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>Concatenated dataframe with the original input and the predictions from the runtime.</p> Source code in <code>adala/skills/base.py</code> <pre><code>def __call__(self, input: InternalDataFrame, runtime: Runtime, dataset: Dataset) -&gt; InternalDataFrame:\n    \"\"\"Calls the runtime to process a batch of inputs. Input and\n    output shapes can be varying, and it should also take care of\n    data types validation\n\n    Args:\n        input (InternalDataFrame): Input data in the form of an InternalDataFrame.\n        runtime (Runtime): The runtime instance to be used for processing.\n        dataset (Dataset): The dataset containing the data to be processed.\n\n    Returns:\n        InternalDataFrame: Concatenated dataframe with the original input and the predictions from the runtime.\n\n    \"\"\"\n\n    # get user defined dataset input fields\n\n    runtime_predictions = runtime.process_batch(\n        batch=input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions=self.instructions,\n        extra_fields=self._get_extra_fields()\n    )\n    runtime_predictions.rename(columns={self.prediction_field: self.name}, inplace=True)\n    output = input.copy()\n    output[runtime_predictions.columns] = runtime_predictions[runtime_predictions.columns]\n    return output\n</code></pre>"},{"location":"skills/#adala.skills.base.BaseSkill.analyze","title":"<code>analyze(predictions, errors, student_runtime, teacher_runtime=None, memory=None)</code>  <code>abstractmethod</code>","text":"<p>Analyzes the results to derive new experiences. It gets provided skill predictions in the format:</p> <pre><code>| input | skill_1 | skill_2 | skill_3 |\n|-------|---------|---------|---------|\n| text1 | label11 | label21 | label31 |\n| text2 | label12 | label22 | label32 |\n| ...   | ...     | ...     | ...     |\n</code></pre> <p>and the errors for a specific skill to analyze in the format:</p> <pre><code>| prediction | ground_truth |\n|------------|--------------|\n| label11    | label12      |\n| ...        | ...          |\n</code></pre> <p>and returns the string that contains the error analysis report.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skill.</p> required <code>errors</code> <code>InternalDataFrame</code> <p>The errors made by the skill.</p> required <code>student_runtime</code> <code>Runtime</code> <p>The runtime instance used to get predictions.</p> required <code>teacher_runtime</code> <code>Optional[Runtime]</code> <p>The runtime instance to be used for analysing the errors.</p> <code>None</code> <code>memory</code> <code>Optional[Memory]</code> <p>The memory instance to be used for processing.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error analysis report.</p> Source code in <code>adala/skills/base.py</code> <pre><code>@abstractmethod\ndef analyze(\n    self,\n    predictions: InternalDataFrame,\n    errors: InternalDataFrame,\n    student_runtime: Runtime,\n    teacher_runtime: Optional[Runtime] = None,\n    memory: Optional[Memory] = None,\n) -&gt; str:\n    \"\"\"\n    Analyzes the results to derive new experiences.\n    It gets provided skill predictions in the format:\n\n    ```markdown\n    | input | skill_1 | skill_2 | skill_3 |\n    |-------|---------|---------|---------|\n    | text1 | label11 | label21 | label31 |\n    | text2 | label12 | label22 | label32 |\n    | ...   | ...     | ...     | ...     |\n    ```\n\n    and the errors for a specific skill to analyze in the format:\n\n    ```markdown\n    | prediction | ground_truth |\n    |------------|--------------|\n    | label11    | label12      |\n    | ...        | ...          |\n    ```\n\n    and returns the string that contains the error analysis report.\n\n    Args:\n        predictions (InternalDataFrame): The predictions made by the skill.\n        errors (InternalDataFrame): The errors made by the skill.\n        student_runtime (Runtime): The runtime instance used to get predictions.\n        teacher_runtime (Optional[Runtime]): The runtime instance to be used for analysing the errors.\n        memory (Optional[Memory]): The memory instance to be used for processing.\n\n    Returns:\n        str: The error analysis report.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.base.BaseSkill.apply","title":"<code>apply(dataset, runtime)</code>  <code>abstractmethod</code>","text":"<p>Applies the skill to a dataset and returns the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset on which the skill is to be applied.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>ShortTermMemory</code> <code>InternalDataFrame</code> <p>The updated experience after applying the skill.</p> Source code in <code>adala/skills/base.py</code> <pre><code>@abstractmethod\ndef apply(\n    self, dataset: Dataset,\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataset and returns the results.\n\n    Args:\n        dataset (Dataset): The dataset on which the skill is to be applied.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        ShortTermMemory: The updated experience after applying the skill.\n    \"\"\"        \n</code></pre>"},{"location":"skills/#adala.skills.base.BaseSkill.improve","title":"<code>improve(error_analysis, runtime)</code>  <code>abstractmethod</code>","text":"<p>Refines the LLM skill based on its recent experiences and updates the skill's instructions.</p> <p>Parameters:</p> Name Type Description Default <code>error_analysis</code> <code>str</code> <p>The error analysis report.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required Source code in <code>adala/skills/base.py</code> <pre><code>@abstractmethod\ndef improve(\n    self,\n    error_analysis: str,\n    runtime: Runtime\n):\n    \"\"\"\n    Refines the LLM skill based on its recent experiences and updates the skill's instructions.\n\n    Args:\n        error_analysis (str): The error analysis report.\n        runtime (Runtime): The runtime instance to be used for processing.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.base.BaseSkill.validate_inputs","title":"<code>validate_inputs()</code>","text":"<p>Validates the input_template, updating it if necessary.</p> <p>Returns:</p> Name Type Description <code>BaseSkill</code> <code>BaseSkill</code> <p>Updated instance of the BaseSkill class.</p> Source code in <code>adala/skills/base.py</code> <pre><code>@model_validator(mode='after')\ndef validate_inputs(self) -&gt; 'BaseSkill':\n    \"\"\"\n    Validates the input_template, updating it if necessary.\n\n    Returns:\n        BaseSkill: Updated instance of the BaseSkill class.\n    \"\"\"\n    if '{{{{{input}}}}}' in self.input_template:\n        if self.input_data_field is None:\n            print_error(f'You provided skill \"{self.name}\" with input template:\\n\\n'\n                        f'{self.__class__.__name__}.input_template = \"{self.input_template}\"\\n\\n'\n                        'that contains \"{{{{{input}}}}}\" placeholder. (yes... 5 curly braces!) \\n\\n'\n                        'In this case, you have to provide skill with `skill.input_data_field` to match the input data.'\n                        f'\\nFor example, if your input data stored in `\"text\"` column, '\n                        f'you can set\\n\\nskill = {self.__class__.__name__}(..., input_data_field=\"text\")')\n            raise ValueError(f'`input_data_field` is not provided for skill {self.name}')\n        self.input_template = self.input_template.format(input=self.input_data_field)\n    return self\n</code></pre>"},{"location":"skills/#adala.skills.base.LLMSkill","title":"<code>LLMSkill</code>","text":"<p>             Bases: <code>BaseSkill</code></p> <p>A skill specialized for Language Models (LLM). Inherits from the BaseSkill  class and provides specific implementations for handling LLM predictions based  on given instructions.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>description</code> <code>str</code> <p>Description of the skill.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>input_data_field</code> <code>str</code> <p>Name of the input data field.</p> <code>prediction_field</code> <code>str</code> <p>Name of the prediction field to be used for the output data.</p> Source code in <code>adala/skills/base.py</code> <pre><code>class LLMSkill(BaseSkill):\n    \"\"\"\n    A skill specialized for Language Models (LLM). Inherits from the BaseSkill \n    class and provides specific implementations for handling LLM predictions based \n    on given instructions.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        description (str): Description of the skill.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        input_data_field (str): Name of the input data field.\n        prediction_field (str): Name of the prediction field to be used for the output data.\n    \"\"\"\n\n    def apply(\n        self,\n        dataset: Union[Dataset, InternalDataFrame],\n        runtime: LLMRuntime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the LLM skill on a dataset and returns the results.\n\n        Args:\n            dataset (Union[Dataset, InternalDataFrame]): The dataset on which the skill is to be applied.\n            runtime (LLMRuntime): The runtime instance to be used for processing.\n\n        Returns:\n            predictions (InternalDataFrame): The predictions made by the skill.\n        \"\"\"\n\n        predictions = []\n        if isinstance(dataset, InternalDataFrame):\n            dataset = DataFrameDataset(df=dataset)\n\n        for batch in dataset.batch_iterator():\n            runtime_predictions = self(batch, runtime, dataset)\n            predictions.append(runtime_predictions)\n\n        if predictions:\n            return InternalDataFrameConcat(predictions, copy=False)\n\n        return InternalDataFrame(columns=dataset.df.columns.tolist() + [self.name])\n\n    def analyze(\n        self,\n        predictions: InternalDataFrame,\n        errors: InternalDataFrame,\n        student_runtime: Runtime,\n        teacher_runtime: Optional[Runtime] = None,\n        memory: Optional[Memory] = None\n    ) -&gt; str:\n        \"\"\"\n        Analyzes the results to identify any discrepancies and returns the observed experience.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            errors (InternalDataFrame): The errors made by the skill.\n            student_runtime (Runtime): The runtime instance used to get predictions.\n            teacher_runtime (Optional[Runtime]): The runtime instance to be used for analysing the errors.\n            memory (Optional[Memory]): The memory instance to be used for processing.\n\n        Returns:\n            str: The error analysis report.\n        \"\"\"\n\n        # collect errors and create error report\n        # first sample errors - make it uniform, but more sophisticated sampling can be implemented\n        MAX_ERRORS = 3\n        errors = errors.sample(n=min(MAX_ERRORS, errors.shape[0]))\n        # TODO: ground truth column name can be the input parameter that comes from GT signal\n        ground_truth_column_name = errors.columns[-1]\n        extra_fields = self._get_extra_fields()\n\n        # get error prepared inputs\n        inputs = student_runtime.process_batch(\n            batch=predictions.loc[errors.index],\n            input_template=self.input_template,\n            extra_fields=extra_fields\n        )\n\n        if not teacher_runtime:\n            teacher_runtime = student_runtime\n\n        predictions_and_errors = pd.concat([\n            inputs,\n            predictions[self.name].loc[errors.index],\n            errors[ground_truth_column_name]\n        ], axis=1)\n        predictions_and_errors.columns = ['input', 'prediction', 'ground_truth']\n        # TODO: move handlebars to Runtime level and abstract template language for skill\n        # For example, using f-string format as generic, that translates to handlebars inside GuidanceRuntime\n        error_reasons = teacher_runtime.process_batch(\n            batch=predictions_and_errors,\n            instructions=\"{{#system~}}\\n\"\n                         \"LLM prompt was created by concatenating instructions with text input:\\n\\n\"\n                         \"Prediction = LLM(Input, Instructions)\\n\\n\"\n                         \"We expect the prediction to be equal to the ground truth.\\n\"\n                         \"Your task is to provide a reason for the error due to the original instruction.\\n\"\n                         \"Be concise and specific.\\n\\n\"\n                         f\"Instructions: {self.instructions}\\n\"\n                         \"{{~/system}}\",\n            input_template=\"{{#user~}}\\n\"\n                           \"{{input}}\\n\"\n                           \"Prediction: {{prediction}}\\n\"\n                           \"Ground truth: {{ground_truth}}\\n\"\n                           \"Error reason:\\n\"\n                           \"{{~/user}}\",\n            output_template=\"{{#assistant~}}{{gen 'reason'}}{{~/assistant}}\",\n            extra_fields=extra_fields\n        )\n        predictions_and_errors['reason'] = error_reasons['reason']\n        # build error report\n        result = teacher_runtime.process_record(\n            record={\n                'predictions_and_errors': predictions_and_errors.to_dict(orient='records'),\n            },\n            input_template=\"{{#each predictions_and_errors}}\"\n                           \"\\n{{this.input}}\\n\"\n                           \"Prediction: {{this.prediction}}\\n\"\n                           \"Ground truth: {{this.ground_truth}}\\n\"\n                           'Error reason: {{this.reason}}\\n'\n                           \"{{/each}}\"\n        )\n        # no specific output specified, all output is in the error report\n        error_report = result['']\n        return error_report\n\n    def improve(\n        self,\n        error_analysis: str,\n        runtime: Runtime,\n    ):\n        \"\"\"\n        Refines the LLM skill based on its recent experiences and updates the skill's instructions.\n\n        Args:\n            error_analysis (str): The error analysis report.\n            runtime (Runtime): The runtime instance to be used for processing.\n        \"\"\"\n\n        result = runtime.process_record(\n            record={\n                'error_analysis': error_analysis\n            },\n            instructions=\"{{#system~}}\\n\"\n                         \"LLM prompt was created by concatenating instructions with text input:\\n\\n\"\n                         \"Prediction = LLM(Input, Instructions)\\n\\n\"\n                         \"We expect the prediction to be equal to the ground truth.\\n\"\n                         \"Your task is to analyze errors made by old instructions \"\n                         \"and craft new instructions for the LLM.\\n\"\n                         \"Follow best practices for LLM prompt engineering.\\n\"\n                         \"Include 2-3 examples at the end of your response to demonstrate how the new instruction would be applied.\\n\"\n                         \"Use the following format for your examples:\\n\"\n                         \"Input: ...\\n\"\n                         \"Output: ...\\n\\n\"\n                         \"{{~/system}}\\n\",\n            input_template=\"{{#user~}}\\n\"\n                           f\"Old instructions: {self.instructions}\\n\\n\"\n                           \"Errors:\\n{{error_analysis}}\\n\"\n                           \"New instruction:\\n\"\n                           \"{{~/user}}\",\n            output_template=\"{{#assistant~}}{{gen 'new_instruction'}}{{~/assistant}}\",\n            extra_fields=self._get_extra_fields()\n        )\n        self.instructions = result['new_instruction']\n</code></pre>"},{"location":"skills/#adala.skills.base.LLMSkill.analyze","title":"<code>analyze(predictions, errors, student_runtime, teacher_runtime=None, memory=None)</code>","text":"<p>Analyzes the results to identify any discrepancies and returns the observed experience.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skill.</p> required <code>errors</code> <code>InternalDataFrame</code> <p>The errors made by the skill.</p> required <code>student_runtime</code> <code>Runtime</code> <p>The runtime instance used to get predictions.</p> required <code>teacher_runtime</code> <code>Optional[Runtime]</code> <p>The runtime instance to be used for analysing the errors.</p> <code>None</code> <code>memory</code> <code>Optional[Memory]</code> <p>The memory instance to be used for processing.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error analysis report.</p> Source code in <code>adala/skills/base.py</code> <pre><code>def analyze(\n    self,\n    predictions: InternalDataFrame,\n    errors: InternalDataFrame,\n    student_runtime: Runtime,\n    teacher_runtime: Optional[Runtime] = None,\n    memory: Optional[Memory] = None\n) -&gt; str:\n    \"\"\"\n    Analyzes the results to identify any discrepancies and returns the observed experience.\n\n    Args:\n        predictions (InternalDataFrame): The predictions made by the skill.\n        errors (InternalDataFrame): The errors made by the skill.\n        student_runtime (Runtime): The runtime instance used to get predictions.\n        teacher_runtime (Optional[Runtime]): The runtime instance to be used for analysing the errors.\n        memory (Optional[Memory]): The memory instance to be used for processing.\n\n    Returns:\n        str: The error analysis report.\n    \"\"\"\n\n    # collect errors and create error report\n    # first sample errors - make it uniform, but more sophisticated sampling can be implemented\n    MAX_ERRORS = 3\n    errors = errors.sample(n=min(MAX_ERRORS, errors.shape[0]))\n    # TODO: ground truth column name can be the input parameter that comes from GT signal\n    ground_truth_column_name = errors.columns[-1]\n    extra_fields = self._get_extra_fields()\n\n    # get error prepared inputs\n    inputs = student_runtime.process_batch(\n        batch=predictions.loc[errors.index],\n        input_template=self.input_template,\n        extra_fields=extra_fields\n    )\n\n    if not teacher_runtime:\n        teacher_runtime = student_runtime\n\n    predictions_and_errors = pd.concat([\n        inputs,\n        predictions[self.name].loc[errors.index],\n        errors[ground_truth_column_name]\n    ], axis=1)\n    predictions_and_errors.columns = ['input', 'prediction', 'ground_truth']\n    # TODO: move handlebars to Runtime level and abstract template language for skill\n    # For example, using f-string format as generic, that translates to handlebars inside GuidanceRuntime\n    error_reasons = teacher_runtime.process_batch(\n        batch=predictions_and_errors,\n        instructions=\"{{#system~}}\\n\"\n                     \"LLM prompt was created by concatenating instructions with text input:\\n\\n\"\n                     \"Prediction = LLM(Input, Instructions)\\n\\n\"\n                     \"We expect the prediction to be equal to the ground truth.\\n\"\n                     \"Your task is to provide a reason for the error due to the original instruction.\\n\"\n                     \"Be concise and specific.\\n\\n\"\n                     f\"Instructions: {self.instructions}\\n\"\n                     \"{{~/system}}\",\n        input_template=\"{{#user~}}\\n\"\n                       \"{{input}}\\n\"\n                       \"Prediction: {{prediction}}\\n\"\n                       \"Ground truth: {{ground_truth}}\\n\"\n                       \"Error reason:\\n\"\n                       \"{{~/user}}\",\n        output_template=\"{{#assistant~}}{{gen 'reason'}}{{~/assistant}}\",\n        extra_fields=extra_fields\n    )\n    predictions_and_errors['reason'] = error_reasons['reason']\n    # build error report\n    result = teacher_runtime.process_record(\n        record={\n            'predictions_and_errors': predictions_and_errors.to_dict(orient='records'),\n        },\n        input_template=\"{{#each predictions_and_errors}}\"\n                       \"\\n{{this.input}}\\n\"\n                       \"Prediction: {{this.prediction}}\\n\"\n                       \"Ground truth: {{this.ground_truth}}\\n\"\n                       'Error reason: {{this.reason}}\\n'\n                       \"{{/each}}\"\n    )\n    # no specific output specified, all output is in the error report\n    error_report = result['']\n    return error_report\n</code></pre>"},{"location":"skills/#adala.skills.base.LLMSkill.apply","title":"<code>apply(dataset, runtime)</code>","text":"<p>Applies the LLM skill on a dataset and returns the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, InternalDataFrame]</code> <p>The dataset on which the skill is to be applied.</p> required <code>runtime</code> <code>LLMRuntime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skill.</p> Source code in <code>adala/skills/base.py</code> <pre><code>def apply(\n    self,\n    dataset: Union[Dataset, InternalDataFrame],\n    runtime: LLMRuntime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the LLM skill on a dataset and returns the results.\n\n    Args:\n        dataset (Union[Dataset, InternalDataFrame]): The dataset on which the skill is to be applied.\n        runtime (LLMRuntime): The runtime instance to be used for processing.\n\n    Returns:\n        predictions (InternalDataFrame): The predictions made by the skill.\n    \"\"\"\n\n    predictions = []\n    if isinstance(dataset, InternalDataFrame):\n        dataset = DataFrameDataset(df=dataset)\n\n    for batch in dataset.batch_iterator():\n        runtime_predictions = self(batch, runtime, dataset)\n        predictions.append(runtime_predictions)\n\n    if predictions:\n        return InternalDataFrameConcat(predictions, copy=False)\n\n    return InternalDataFrame(columns=dataset.df.columns.tolist() + [self.name])\n</code></pre>"},{"location":"skills/#adala.skills.base.LLMSkill.improve","title":"<code>improve(error_analysis, runtime)</code>","text":"<p>Refines the LLM skill based on its recent experiences and updates the skill's instructions.</p> <p>Parameters:</p> Name Type Description Default <code>error_analysis</code> <code>str</code> <p>The error analysis report.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required Source code in <code>adala/skills/base.py</code> <pre><code>def improve(\n    self,\n    error_analysis: str,\n    runtime: Runtime,\n):\n    \"\"\"\n    Refines the LLM skill based on its recent experiences and updates the skill's instructions.\n\n    Args:\n        error_analysis (str): The error analysis report.\n        runtime (Runtime): The runtime instance to be used for processing.\n    \"\"\"\n\n    result = runtime.process_record(\n        record={\n            'error_analysis': error_analysis\n        },\n        instructions=\"{{#system~}}\\n\"\n                     \"LLM prompt was created by concatenating instructions with text input:\\n\\n\"\n                     \"Prediction = LLM(Input, Instructions)\\n\\n\"\n                     \"We expect the prediction to be equal to the ground truth.\\n\"\n                     \"Your task is to analyze errors made by old instructions \"\n                     \"and craft new instructions for the LLM.\\n\"\n                     \"Follow best practices for LLM prompt engineering.\\n\"\n                     \"Include 2-3 examples at the end of your response to demonstrate how the new instruction would be applied.\\n\"\n                     \"Use the following format for your examples:\\n\"\n                     \"Input: ...\\n\"\n                     \"Output: ...\\n\\n\"\n                     \"{{~/system}}\\n\",\n        input_template=\"{{#user~}}\\n\"\n                       f\"Old instructions: {self.instructions}\\n\\n\"\n                       \"Errors:\\n{{error_analysis}}\\n\"\n                       \"New instruction:\\n\"\n                       \"{{~/user}}\",\n        output_template=\"{{#assistant~}}{{gen 'new_instruction'}}{{~/assistant}}\",\n        extra_fields=self._get_extra_fields()\n    )\n    self.instructions = result['new_instruction']\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet","title":"<code>LinearSkillSet</code>","text":"<p>             Bases: <code>SkillSet</code></p> <p>Represents a sequence of skills that are acquired in a specific order to achieve a goal.</p> <p>LinearSkillSet ensures that skills are developed in a sequential manner, determined either  by the provided skill_sequence or by the lexicographical order of skill names.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Union[List[str], Dict[str, str], List[BaseSkill], Dict[str, BaseSkill]]</code> <p>Provided skills</p> <code>skill_sequence</code> <code>List[str]</code> <p>Ordered list of skill names indicating the order                                    in which they should be acquired.                                   By default, lexographical order of skill names is used.</p> <code>input_data_field</code> <code>Optional[str]</code> <p>Name of the input data field. Defaults to None.</p> <p>Examples:</p> <p>Create a LinearSkillSet with a list of skills specified as strings:</p> <pre><code>&gt;&gt;&gt; from adala.skills import LinearSkillSet\n&gt;&gt;&gt; skillset = LinearSkillSet(skills=['Extract keywords', 'Classify keywords', 'Create structured output'])\n</code></pre> <p>Create a LinearSkillSet with a list of skills specified as BaseSkill instances:</p> <pre><code>&gt;&gt;&gt; from adala.skills import LinearSkillSet, TextGenerationSkill\n&gt;&gt;&gt; skillset = LinearSkillSet(skills=[TextGenerationSkill(name='Generate text', instructions='Generate text from keywords'),])\n</code></pre> <p>Create a LinearSkillSet with a dictionary of skill names to instructions:</p> <pre><code>&gt;&gt;&gt; from adala.skills import LinearSkillSet\n&gt;&gt;&gt; skillset = LinearSkillSet(skills={'extract': 'Extract keywords from text', 'classify': 'Classify keywords', 'structured_output': 'Create structured output from keywords'})\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class LinearSkillSet(SkillSet):\n    \"\"\"\n    Represents a sequence of skills that are acquired in a specific order to achieve a goal.\n\n    LinearSkillSet ensures that skills are developed in a sequential manner, determined either \n    by the provided skill_sequence or by the lexicographical order of skill names.\n\n    Attributes:\n        skills (Union[List[str], Dict[str, str], List[BaseSkill], Dict[str, BaseSkill]]): Provided skills\n        skill_sequence (List[str], optional): Ordered list of skill names indicating the order \n                                              in which they should be acquired.\n                                              By default, lexographical order of skill names is used.\n        input_data_field (Optional[str], optional): Name of the input data field. Defaults to None.\n\n    Examples:\n        Create a LinearSkillSet with a list of skills specified as strings:\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet\n        &gt;&gt;&gt; skillset = LinearSkillSet(skills=['Extract keywords', 'Classify keywords', 'Create structured output'])\n\n        Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TextGenerationSkill\n        &gt;&gt;&gt; skillset = LinearSkillSet(skills=[TextGenerationSkill(name='Generate text', instructions='Generate text from keywords'),])\n\n        Create a LinearSkillSet with a dictionary of skill names to instructions:\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet\n        &gt;&gt;&gt; skillset = LinearSkillSet(skills={'extract': 'Extract keywords from text', 'classify': 'Classify keywords', 'structured_output': 'Create structured output from keywords'})\n    \"\"\"\n\n    skill_sequence: List[str] = None\n    input_data_field: Optional[str] = None\n\n    @field_validator('skills', mode='before')\n    def skills_validator(cls, v: Union[List[str], List[BaseSkill], Dict[str, BaseSkill]]) -&gt; Dict[str, BaseSkill]:\n        \"\"\"\n        Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n        Args:\n            v (Union[List[str], List[BaseSkill], Dict[str, BaseSkill]]): The skills attribute to validate.\n\n        Returns:\n            Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n        \"\"\"\n        skills = OrderedDict()\n        if not v:\n            return skills\n\n        input_data_field = None\n        if isinstance(v, list) and isinstance(v[0], str):\n            # if list of strings presented, they are interpreted as skill instructions\n            for i, instructions in enumerate(v):\n                skill_name = f\"skill_{i}\"\n                skills[skill_name] = LLMSkill(\n                    name=skill_name,\n                    instructions=instructions,\n                    input_data_field=input_data_field\n                )\n                # Linear skillset creates skills pipeline - update input_data_field for next skill\n                input_data_field = skill_name\n        elif isinstance(v, dict) and isinstance(v[list(v.keys())[0]], str):\n            # if dictionary of strings presented, they are interpreted as skill instructions\n            for skill_name, instructions in v.items():\n                skills[skill_name] = LLMSkill(\n                    name=skill_name,\n                    instructions=instructions,\n                    input_data_field=input_data_field\n                )\n                # Linear skillset creates skills pipeline - update input_data_field for next skill\n                input_data_field = skill_name\n        elif isinstance(v, list) and isinstance(v[0], BaseSkill):\n            # convert list of skill names to dictionary\n            for skill in v:\n                skills[skill.name] = skill\n        elif isinstance(v, dict):\n            skills = v\n        else:\n            raise ValueError(f\"skills must be a list or dictionary, not {type(skills)}\")\n        return skills\n\n    @model_validator(mode='after')\n    def skill_sequence_validator(self) -&gt; 'LinearSkillSet':\n        \"\"\"\n        Validates and sets the default order for the skill sequence if not provided.\n\n        Returns:\n            LinearSkillSet: The current instance with updated skill_sequence attribute.\n        \"\"\"\n        if self.skill_sequence is None:\n            # use default skill sequence defined by lexicographical order\n            self.skill_sequence = list(self.skills.keys())\n        if len(self.skill_sequence) != len(self.skills):\n            raise ValueError(f\"skill_sequence must contain all skill names - \"\n                             f\"length of skill_sequence is {len(self.skill_sequence)} \"\n                             f\"while length of skills is {len(self.skills)}\")\n        return self\n\n    def apply(\n        self,\n        dataset: Union[Dataset, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Sequentially applies each skill on the dataset, enhancing the agent's experience.\n\n        Args:\n            dataset (Dataset): The dataset to apply the skills on.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n\n        predictions = None\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = self.skill_sequence[self.skill_sequence.index(improved_skill):]\n        else:\n            skill_sequence = self.skill_sequence\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            input_dataset = dataset if i == 0 else predictions\n            print_text(f\"Applying skill: {skill_name}\")\n            predictions = skill.apply(input_dataset, runtime)\n\n        return predictions\n\n    def select_skill_to_improve(\n        self,\n        accuracy: Mapping,\n        accuracy_threshold: Optional[float] = 1.0\n    ) -&gt; Optional[BaseSkill]:\n        \"\"\"\n        Selects the skill with the lowest accuracy to improve.\n\n        Args:\n            accuracy (Mapping): Accuracy of each skill.\n            accuracy_threshold (Optional[float], optional): Accuracy threshold. Defaults to 1.0.\n        Returns:\n            Optional[BaseSkill]: Skill to improve. None if no skill to improve.\n        \"\"\"\n        for skill_name in self.skill_sequence:\n            if accuracy[skill_name] &lt; accuracy_threshold:\n                return self.skills[skill_name]\n\n    def __rich__(self):\n        \"\"\"Returns a rich representation of the skill.\"\"\"\n        # TODO: move it to a base class and use repr derived from Skills\n        text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n        for skill in self.skills.values():\n            text += f'[bold underline green]{skill.name}[/bold underline green]\\n' \\\n                    f'[green]{skill.instructions}[green]\\n'\n        return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a rich representation of the skill.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __rich__(self):\n    \"\"\"Returns a rich representation of the skill.\"\"\"\n    # TODO: move it to a base class and use repr derived from Skills\n    text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n    for skill in self.skills.values():\n        text += f'[bold underline green]{skill.name}[/bold underline green]\\n' \\\n                f'[green]{skill.instructions}[green]\\n'\n    return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.apply","title":"<code>apply(dataset, runtime, improved_skill=None)</code>","text":"<p>Sequentially applies each skill on the dataset, enhancing the agent's experience.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to apply the skills on.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to improve. Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    dataset: Union[Dataset, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Sequentially applies each skill on the dataset, enhancing the agent's experience.\n\n    Args:\n        dataset (Dataset): The dataset to apply the skills on.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n\n    predictions = None\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = self.skill_sequence[self.skill_sequence.index(improved_skill):]\n    else:\n        skill_sequence = self.skill_sequence\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        input_dataset = dataset if i == 0 else predictions\n        print_text(f\"Applying skill: {skill_name}\")\n        predictions = skill.apply(input_dataset, runtime)\n\n    return predictions\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.select_skill_to_improve","title":"<code>select_skill_to_improve(accuracy, accuracy_threshold=1.0)</code>","text":"<p>Selects the skill with the lowest accuracy to improve.</p> <p>Parameters:</p> Name Type Description Default <code>accuracy</code> <code>Mapping</code> <p>Accuracy of each skill.</p> required <code>accuracy_threshold</code> <code>Optional[float]</code> <p>Accuracy threshold. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:     Optional[BaseSkill]: Skill to improve. None if no skill to improve.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def select_skill_to_improve(\n    self,\n    accuracy: Mapping,\n    accuracy_threshold: Optional[float] = 1.0\n) -&gt; Optional[BaseSkill]:\n    \"\"\"\n    Selects the skill with the lowest accuracy to improve.\n\n    Args:\n        accuracy (Mapping): Accuracy of each skill.\n        accuracy_threshold (Optional[float], optional): Accuracy threshold. Defaults to 1.0.\n    Returns:\n        Optional[BaseSkill]: Skill to improve. None if no skill to improve.\n    \"\"\"\n    for skill_name in self.skill_sequence:\n        if accuracy[skill_name] &lt; accuracy_threshold:\n            return self.skills[skill_name]\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.skill_sequence_validator","title":"<code>skill_sequence_validator()</code>","text":"<p>Validates and sets the default order for the skill sequence if not provided.</p> <p>Returns:</p> Name Type Description <code>LinearSkillSet</code> <code>LinearSkillSet</code> <p>The current instance with updated skill_sequence attribute.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@model_validator(mode='after')\ndef skill_sequence_validator(self) -&gt; 'LinearSkillSet':\n    \"\"\"\n    Validates and sets the default order for the skill sequence if not provided.\n\n    Returns:\n        LinearSkillSet: The current instance with updated skill_sequence attribute.\n    \"\"\"\n    if self.skill_sequence is None:\n        # use default skill sequence defined by lexicographical order\n        self.skill_sequence = list(self.skills.keys())\n    if len(self.skill_sequence) != len(self.skills):\n        raise ValueError(f\"skill_sequence must contain all skill names - \"\n                         f\"length of skill_sequence is {len(self.skill_sequence)} \"\n                         f\"while length of skills is {len(self.skills)}\")\n    return self\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[List[str], List[BaseSkill], Dict[str, BaseSkill]]</code> <p>The skills attribute to validate.</p> required <p>Returns:</p> Type Description <code>Dict[str, BaseSkill]</code> <p>Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@field_validator('skills', mode='before')\ndef skills_validator(cls, v: Union[List[str], List[BaseSkill], Dict[str, BaseSkill]]) -&gt; Dict[str, BaseSkill]:\n    \"\"\"\n    Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n    Args:\n        v (Union[List[str], List[BaseSkill], Dict[str, BaseSkill]]): The skills attribute to validate.\n\n    Returns:\n        Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n    \"\"\"\n    skills = OrderedDict()\n    if not v:\n        return skills\n\n    input_data_field = None\n    if isinstance(v, list) and isinstance(v[0], str):\n        # if list of strings presented, they are interpreted as skill instructions\n        for i, instructions in enumerate(v):\n            skill_name = f\"skill_{i}\"\n            skills[skill_name] = LLMSkill(\n                name=skill_name,\n                instructions=instructions,\n                input_data_field=input_data_field\n            )\n            # Linear skillset creates skills pipeline - update input_data_field for next skill\n            input_data_field = skill_name\n    elif isinstance(v, dict) and isinstance(v[list(v.keys())[0]], str):\n        # if dictionary of strings presented, they are interpreted as skill instructions\n        for skill_name, instructions in v.items():\n            skills[skill_name] = LLMSkill(\n                name=skill_name,\n                instructions=instructions,\n                input_data_field=input_data_field\n            )\n            # Linear skillset creates skills pipeline - update input_data_field for next skill\n            input_data_field = skill_name\n    elif isinstance(v, list) and isinstance(v[0], BaseSkill):\n        # convert list of skill names to dictionary\n        for skill in v:\n            skills[skill.name] = skill\n    elif isinstance(v, dict):\n        skills = v\n    else:\n        raise ValueError(f\"skills must be a list or dictionary, not {type(skills)}\")\n    return skills\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet","title":"<code>ParallelSkillSet</code>","text":"<p>             Bases: <code>SkillSet</code></p> <p>Represents a set of skills that are acquired simultaneously to reach a goal.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>class ParallelSkillSet(SkillSet):\n    \"\"\"\n    Represents a set of skills that are acquired simultaneously to reach a goal.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet","title":"<code>SkillSet</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a collection of interdependent skills aiming to achieve a specific goal.</p> <p>A skill set breaks down the path to achieve a goal into necessary precursor skills. Agents can evolve these skills either in parallel for tasks like self-consistency or  sequentially for complex problem decompositions and causal reasoning. In the most generic cases, task decomposition can involve a graph-based approach.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Dict[str, BaseSkill]</code> <p>A dictionary of skills in the skill set.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>class SkillSet(BaseModel, ABC):\n    \"\"\"\n    Represents a collection of interdependent skills aiming to achieve a specific goal.\n\n    A skill set breaks down the path to achieve a goal into necessary precursor skills.\n    Agents can evolve these skills either in parallel for tasks like self-consistency or \n    sequentially for complex problem decompositions and causal reasoning. In the most generic\n    cases, task decomposition can involve a graph-based approach.\n\n    Attributes:\n        skills (Dict[str, BaseSkill]): A dictionary of skills in the skill set.\n    \"\"\"\n\n    skills: Dict[str, BaseSkill]\n\n    @abstractmethod\n    def apply(\n        self,\n        dataset: Union[Dataset, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Apply the skill set to a dataset using a specified runtime.\n\n        Args:\n            dataset (Union[Dataset, InternalDataFrame]): The dataset to apply the skill set to.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n\n    @abstractmethod\n    def select_skill_to_improve(self, accuracy: Mapping, accuracy_threshold: Optional[float] = 1.0) -&gt; Optional[BaseSkill]:\n        \"\"\"\n        Select skill to improve based on accuracy.\n\n        Args:\n            accuracy (Mapping): Skills accuracies.\n            accuracy_threshold (Optional[float], optional): Accuracy threshold. Defaults to 1.0.\n        Returns:\n            Optional[BaseSkill]: Skill to improve. None if no skill to improve.\n        \"\"\"\n\n    def __getitem__(self, skill_name) -&gt; BaseSkill:\n        \"\"\"\n        Select skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to select.\n\n        Returns:\n            BaseSkill: Skill\n        \"\"\"\n        return self.skills[skill_name]\n\n    def __setitem__(self, skill_name, skill: BaseSkill):\n        \"\"\"\n        Set skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to set.\n            skill (BaseSkill): Skill to set.\n        \"\"\"\n        self.skills[skill_name] = skill\n\n    def get_skill_names(self) -&gt; List[str]:\n        \"\"\"\n        Get list of skill names.\n\n        Returns:\n            List[str]: List of skill names.\n        \"\"\"\n        return list(self.skills.keys())\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__getitem__","title":"<code>__getitem__(skill_name)</code>","text":"<p>Select skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to select.</p> required <p>Returns:</p> Name Type Description <code>BaseSkill</code> <code>BaseSkill</code> <p>Skill</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __getitem__(self, skill_name) -&gt; BaseSkill:\n    \"\"\"\n    Select skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to select.\n\n    Returns:\n        BaseSkill: Skill\n    \"\"\"\n    return self.skills[skill_name]\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__setitem__","title":"<code>__setitem__(skill_name, skill)</code>","text":"<p>Set skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to set.</p> required <code>skill</code> <code>BaseSkill</code> <p>Skill to set.</p> required Source code in <code>adala/skills/skillset.py</code> <pre><code>def __setitem__(self, skill_name, skill: BaseSkill):\n    \"\"\"\n    Set skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to set.\n        skill (BaseSkill): Skill to set.\n    \"\"\"\n    self.skills[skill_name] = skill\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.apply","title":"<code>apply(dataset, runtime, improved_skill=None)</code>  <code>abstractmethod</code>","text":"<p>Apply the skill set to a dataset using a specified runtime.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, InternalDataFrame]</code> <p>The dataset to apply the skill set to.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to start from (to optimize calculations). Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@abstractmethod\ndef apply(\n    self,\n    dataset: Union[Dataset, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Apply the skill set to a dataset using a specified runtime.\n\n    Args:\n        dataset (Union[Dataset, InternalDataFrame]): The dataset to apply the skill set to.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_names","title":"<code>get_skill_names()</code>","text":"<p>Get list of skill names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of skill names.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_names(self) -&gt; List[str]:\n    \"\"\"\n    Get list of skill names.\n\n    Returns:\n        List[str]: List of skill names.\n    \"\"\"\n    return list(self.skills.keys())\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.select_skill_to_improve","title":"<code>select_skill_to_improve(accuracy, accuracy_threshold=1.0)</code>  <code>abstractmethod</code>","text":"<p>Select skill to improve based on accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>accuracy</code> <code>Mapping</code> <p>Skills accuracies.</p> required <code>accuracy_threshold</code> <code>Optional[float]</code> <p>Accuracy threshold. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:     Optional[BaseSkill]: Skill to improve. None if no skill to improve.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@abstractmethod\ndef select_skill_to_improve(self, accuracy: Mapping, accuracy_threshold: Optional[float] = 1.0) -&gt; Optional[BaseSkill]:\n    \"\"\"\n    Select skill to improve based on accuracy.\n\n    Args:\n        accuracy (Mapping): Skills accuracies.\n        accuracy_threshold (Optional[float], optional): Accuracy threshold. Defaults to 1.0.\n    Returns:\n        Optional[BaseSkill]: Skill to improve. None if no skill to improve.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.generation.base.TextGenerationSkill","title":"<code>TextGenerationSkill</code>","text":"<p>             Bases: <code>LLMSkill</code></p> <p>Skill specialized for generating text based on the provided input.</p> <p>This involves tasks where the LLM is expected to produce creative, coherent, and contextually  relevant textual content based on the given input.</p> <p>Attributes:</p> Name Type Description <code>instructions</code> <code>str</code> <p>Instruction to guide the LLM in text generation.</p> Source code in <code>adala/skills/generation/base.py</code> <pre><code>class TextGenerationSkill(LLMSkill):\n    \"\"\"\n    Skill specialized for generating text based on the provided input.\n\n    This involves tasks where the LLM is expected to produce creative, coherent, and contextually \n    relevant textual content based on the given input.\n\n    Attributes:\n        instructions (str): Instruction to guide the LLM in text generation.\n    \"\"\"\n\n    instructions: str = 'Generate text based on the provided input.'\n</code></pre>"},{"location":"skills/#adala.skills.generation.qa.QuestionAnsweringSkill","title":"<code>QuestionAnsweringSkill</code>","text":"<p>             Bases: <code>TextGenerationSkill</code></p> <p>Skill specialized for answering questions based on the provided input.</p> <p>Inherits from the TextGenerationSkill and focuses on generating answers to the questions  posed in the input. The class customizes the instructions, input, and output templates  specifically for question-answering tasks.</p> <p>Attributes:</p> Name Type Description <code>instructions</code> <code>str</code> <p>Instruction to guide the LLM in answering the question.</p> <code>input_template</code> <code>str</code> <p>Format in which the question is presented to the LLM.</p> <code>output_template</code> <code>str</code> <p>Expected format of the LLM's answer.</p> <code>prediction_field</code> <code>str</code> <p>Field name for the generated answer.</p> Source code in <code>adala/skills/generation/qa.py</code> <pre><code>class QuestionAnsweringSkill(TextGenerationSkill):\n    \"\"\"\n    Skill specialized for answering questions based on the provided input.\n\n    Inherits from the TextGenerationSkill and focuses on generating answers to the questions \n    posed in the input. The class customizes the instructions, input, and output templates \n    specifically for question-answering tasks.\n\n    Attributes:\n        instructions (str): Instruction to guide the LLM in answering the question.\n        input_template (str): Format in which the question is presented to the LLM.\n        output_template (str): Expected format of the LLM's answer.\n        prediction_field (str): Field name for the generated answer.\n    \"\"\"\n\n    instructions: str = 'Answer the question.'\n    input_template: str = \"Question: {{{{{input}}}}}\"\n    output_template: str = \"Answer: {{gen 'answer'}}\"\n    prediction_field: str = 'answer'\n</code></pre>"},{"location":"skills/#adala.skills.generation.summarization.SummarizationSkill","title":"<code>SummarizationSkill</code>","text":"<p>             Bases: <code>TextGenerationSkill</code></p> <p>Skill specialized for summarizing lengthy texts based on the provided input.</p> <p>Inherits from the TextGenerationSkill and focuses on generating concise summaries  for the input texts. The class customizes the instructions, input, and output templates  specifically for text summarization tasks.</p> <p>Attributes:</p> Name Type Description <code>instructions</code> <code>str</code> <p>Instruction to guide the LLM in summarizing the text.</p> <code>input_template</code> <code>str</code> <p>Format in which the full text is presented to the LLM.</p> <code>output_template</code> <code>str</code> <p>Expected format of the LLM's summary.</p> <code>prediction_field</code> <code>str</code> <p>Field name for the generated summary.</p> Source code in <code>adala/skills/generation/summarization.py</code> <pre><code>class SummarizationSkill(TextGenerationSkill):\n    \"\"\"\n    Skill specialized for summarizing lengthy texts based on the provided input.\n\n    Inherits from the TextGenerationSkill and focuses on generating concise summaries \n    for the input texts. The class customizes the instructions, input, and output templates \n    specifically for text summarization tasks.\n\n    Attributes:\n        instructions (str): Instruction to guide the LLM in summarizing the text.\n        input_template (str): Format in which the full text is presented to the LLM.\n        output_template (str): Expected format of the LLM's summary.\n        prediction_field (str): Field name for the generated summary.\n    \"\"\"\n\n    instructions: str = 'Summarize the text.'\n    input_template: str = \"Text: {{{{{input}}}}}\"\n    output_template: str = \"Summary: {{gen 'summary'}}\"\n    prediction_field: str = 'summary'\n</code></pre>"},{"location":"skills/#adala.skills.labeling.classification.ClassificationSkill","title":"<code>ClassificationSkill</code>","text":"<p>             Bases: <code>LLMSkill</code></p> <p>Skill specialized for classifying text inputs based on a predefined set of labels.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>description</code> <code>str</code> <p>Description of the skill.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>input_data_field</code> <code>str</code> <p>Name of the input data field.</p> <code>prediction_field</code> <code>str</code> <p>Name of the prediction field to be used for the output data.</p> <code>labels</code> <code>List[str]</code> <p>A list of valid labels for the classification task.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; skill = ClassificationSkill(labels=['positive', 'negative'])\n&gt;&gt;&gt; skill.apply(pd.DataFrame({'text': ['I love this movie!', 'I hate this movie!']}))\ntext                    predictions score\nI love this movie!      positive   0.99\nI hate this movie!      negative   0.99\n</code></pre> Source code in <code>adala/skills/labeling/classification.py</code> <pre><code>class ClassificationSkill(LLMSkill):\n    \"\"\"\n    Skill specialized for classifying text inputs based on a predefined set of labels.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        description (str): Description of the skill.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        input_data_field (str): Name of the input data field.\n        prediction_field (str): Name of the prediction field to be used for the output data.\n        labels (List[str]): A list of valid labels for the classification task.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; skill = ClassificationSkill(labels=['positive', 'negative'])\n        &gt;&gt;&gt; skill.apply(pd.DataFrame({'text': ['I love this movie!', 'I hate this movie!']}))\n        text                    predictions score\n        I love this movie!      positive   0.99\n        I hate this movie!      negative   0.99\n    \"\"\"\n\n    instructions: str = 'Label the input text with the following labels: {{labels}}'\n    labels: List[str]\n    output_template: str = \"Output: {{select 'predictions' options=labels logprobs='score'}}\"\n    prediction_field: str = 'predictions'\n</code></pre>"},{"location":"skills/#adala.skills.labeling.classification.ClassificationSkillWithCoT","title":"<code>ClassificationSkillWithCoT</code>","text":"<p>             Bases: <code>ClassificationSkill</code></p> <p>Skill specialized for classifying text inputs with the addition of generating a Chain of Thought.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>description</code> <code>str</code> <p>Description of the skill.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>input_data_field</code> <code>str</code> <p>Name of the input data field.</p> <code>prediction_field</code> <code>str</code> <p>Name of the prediction field to be used for the output data.</p> <code>labels</code> <code>List[str]</code> <p>A list of valid labels for the classification task.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; skill = ClassificationSkillWithCoT(labels=['positive', 'negative'])\n&gt;&gt;&gt; skill.apply(pd.DataFrame({'text': ['I love this movie!', 'I hate this movie!']}))\ntext                    predictions score  rationale\nI love this movie!      positive   0.99    classified as positive because I love this movie!\nI hate this movie!      negative   0.99    classified as negative because I hate this movie!\n</code></pre> Source code in <code>adala/skills/labeling/classification.py</code> <pre><code>class ClassificationSkillWithCoT(ClassificationSkill):\n    \"\"\"\n    Skill specialized for classifying text inputs with the addition of generating a Chain of Thought.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        description (str): Description of the skill.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        input_data_field (str): Name of the input data field.\n        prediction_field (str): Name of the prediction field to be used for the output data.\n        labels (List[str]): A list of valid labels for the classification task.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; skill = ClassificationSkillWithCoT(labels=['positive', 'negative'])\n        &gt;&gt;&gt; skill.apply(pd.DataFrame({'text': ['I love this movie!', 'I hate this movie!']}))\n        text                    predictions score  rationale\n        I love this movie!      positive   0.99    classified as positive because I love this movie!\n        I hate this movie!      negative   0.99    classified as negative because I hate this movie!\n    \"\"\"\n\n    instructions: str = 'Label the input text with the following labels: {{labels}}. Provide a rationale for your answer.'\n    output_template: str = \"Thoughts: {{gen 'rationale'}}\\nOutput: {{select 'predictions' options=labels logprobs='score'}}\"\n</code></pre>"},{"location":"utils/","title":"Utils","text":""}]}